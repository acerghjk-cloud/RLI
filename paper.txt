%\documentclass[preprint,12pt]{elsarticle}
\documentclass[times, review, 10pt]{elsarticle}

% ===================================================
% usepackage
% ===================================================
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{url}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{soul}
\usepackage{siunitx}
\definecolor{luckyred}{rgb}{1, 0, 0}
\usepackage{caption}
\usepackage{array}
\usepackage{pifont}
\usepackage{float}


\usepackage{tabularx}



\newcommand{\textbfc}[2]{\textbf{\textcolor{#1}{#2}}}

% ===================================================
% INFO
% ===================================================
\journal{Pattern Recognition}
\begin{document}




\begin{frontmatter}
\title{Robust Defect Image Synthesis Using Null Embedding Optimization for Industrial Applications}
% [박인규] 제목이 이상해서 수정함
% \author[inha]{Hyunwook Jo}
\author{Hyunwook Jo}
\ead{acerghjk@inha.edu}
\author{Jun Hyung Park}
\ead{kevin2001112@inha.edu}
\author{In Kyu Park\corref{cor1}}
\cortext[cor1]{Corresponding author}
\ead{pik@inha.ac.kr}




\affiliation{
  organization={Department of Electrical and Computer Engineering, Inha University\\
  Incheon 22212},  % [박인규] 이정도로 충분
%  addressline={100 Inha-ro, Michuhol-gu}, 
%  city={Incheon}, 
%  postcode={22212}, 
%  state={Incheon 22212}, 
  country={Republic of Korea}
}

%% Abstract

\begin{abstract}
Accurate defect classification and segmentation are critical in the manufacturing sector, yet both tasks are often hindered by imbalanced data and the scarcity of defect samples. Traditional synthetic data augmentation methods tend to produce images with structural inconsistencies, limiting their effectiveness. In this work, we introduce a novel approach that integrates null embedding optimization with Residual Linear Interpolation~(RLI) connections to generate latent representations that closely mimic the original images while preserving structural fidelity. Furthermore, a prompt-to-prompt augmentation technique is employed to systematically modify the base text prompt, enabling the generation of diverse defect morphologies. \textcolor{luckyred}{This unified framework primarily enhances the variability of the dataset by generating diverse defect morphologies, while simultaneously yielding high-fidelity synthetic images that visually correspond to real defects, thereby significantly improving the performance of both classification and segmentation models. The source code and models are available at \url{https://acerghjk-cloud.github.io/PR2025/}}
\end{abstract}

%% Keywords
\begin{keyword}
Data augmentation \sep defect classification \sep defect segmentation \sep diffusion \sep image synthesis \sep image editing
\end{keyword}

\end{frontmatter}


\newpage

% ===================================================
% Introduction
% ===================================================
\section{Introduction}
Defect detection is a critical task in manufacturing. Despite extensive efforts to improve production quality, the collection and accurate labeling of defect samples remains a challenge because of their inherent scarcity. As computer vision technologies advance, data-centric deep learning approaches have become the norm. Training robust models for defect classification and segmentation requires both normal and defect samples, but acquiring well-labeled defect data is often difficult in practice. Moreover, generative models frequently produce images with structural inconsistencies~\cite{inconsistency}, leading to synthesized outputs that do not faithfully represent actual defects. Consequently, significant research has focused on generating synthetic defects~\cite{eunhee, pr_data_aug_diff, pr_data_aug_gan}. 
\textcolor{luckyred}{We posit} that for real industrial applications, synthetic defect samples must meet three stringent criteria: they should closely resemble actual defects, be generated without structural inconsistencies~\cite{comment1}, and be suitable for automatic labeling. In defect classification, where detailed mask annotations are not required, minor variations in the synthesized defects can be tolerated. In contrast, for defect segmentation where pixel-level precision is essential, even subtle differences in defect characteristics demand distinct mask labels, making re-labeling necessary to capture these nuanced variations.

\textcolor{luckyred}{We contend} that generating defects without structural inconsistency is essential to enable automatic labeling. However, a review of the various generative models currently in use reveals a significant contradiction. Although these models succeed in producing outputs that closely mimic real defects, they frequently fail to generate defects without structural inconsistencies due to the inherent stochastic nature of generative algorithms, which are typically based on Gaussian noise~\cite{ddpm,gan,sd}. This randomness allows the models to perform adequately but also makes them difficult to control, as using the same noise vector repeatedly can yield different images each time. This inconsistency is a direct consequence of the stochastic behavior of generative algorithms and the challenges in managing Gaussian noise.

This issue prompts a critical question\textcolor{luckyred}{:} How can we \textcolor{luckyred}{rigorously mitigate structural inconsistencies}? To address this, we propose a methodology that closely mirrors the original product while maintaining a high peak signal-to-noise ratio (PSNR). Our preliminary work~\cite{DISN} enhances existing methods by incorporating LoRA weights~\cite{lora} with Stable Diffusion XL~(SDXL)~\cite{sdxl} and optimizing both null embedding and null pool embedding to achieve an optimal latent representation, thus generating defects that closely resemble the original. In summary, we introduce null embedding optimization to capture the same latent space as the original, generate a diverse set of defect samples through attention map blending and amplification, and demonstrate that using synthetically generated images significantly improves the accuracy of off-the-shelf defect classification models.

Building on our previous work, we further extend our methodology to address both defect classification and segmentation. Specifically, we improve the UNet architecture by incorporating Residual Linear Interpolation~(RLI) connections, which help preserve fine-grained structural details. Importantly, our approach eliminates the need for LoRA training by relying solely on an optimization-based refinement of the latent representations within the SDXL model. The experimental results confirm that our refined approach generates high-quality defect samples that not only boost classification accuracy but also enhance the precision of segmentation models, offering a comprehensive solution for defect detection in real-world industrial applications.




% ===================================================
% Related Works
% ===================================================
\section{Related Works}
\subsection{Defect Detection}
\textcolor{luckyred}{Defect detection is a pivotal component in industrial visual inspection, encompassing critical tasks such as defect classification and segmentation. With the emergence of deep learning, these techniques have achieved remarkable success in various real-world scenarios, including industrial manufacturing, safety inspections, and structural evaluations~\cite{deep4,deep5,deep6}. Unlike traditional methods that rely on manual feature extraction, deep learning-based approaches enable automatic object classification~\cite{deep1,deep3} and simplify algorithm development. Furthermore, advanced methodologies allow for the precise delineation of defective regions at the pixel level, which is essential for meticulous quality control~\cite{seg_metal,seg_wafer,seg_pcb}. State-of-the-art models, leveraging architectures ranging from convolutional neural networks to transformer-based systems, effectively capture multi-scale contextual features, thereby reducing human errors and minimizing overall defect rates. However, despite these advancements, obtaining a sufficient number of defect samples remains a significant challenge in industrial applications due to the highly imbalanced nature of real world datasets~\cite{bai2024surface}.}

\subsection{Synthetic Generation using GAN}
\textcolor{luckyred}{Prior to the dominance of diffusion models, Generative Adversarial Networks (GANs) were widely investigated for synthetic defect generation~\cite{defect-gan,eunhee, pr_data_aug_gan}. Approaches such as Defect-GAN~\cite{defect-gan} and SyNDGAN~\cite{eunhee} aimed to improve data diversity and classification performance. However, GAN-based methods often face challenges regarding training instability and the requirement for large-scale datasets to avoid mode collapse~\cite{gan}. Consequently, the research focus has significantly shifted toward diffusion models, which offer superior generation quality and stability.}

\subsection{Synthetic Generation using Diffusion}
\textcolor{luckyred}{Diffusion models~\cite{sdxl, sd} have demonstrated remarkable capabilities in generating high-quality images, leading to their adoption in defect synthesis tasks~\cite{stablesdg, pr_data_aug_diff}. Recent advancements in this domain can be categorized into personalization, inpainting, and inversion techniques.}

\textcolor{luckyred}{\textbf{Personalization Methods.} Techniques such as DreamBooth~\cite{dreambooth} and LoRA~\cite{lora} facilitate the personalization of text-to-image models by fine-tuning weights with a few reference images. While effective for general style transfer, these methods often suffer from \textit{structural inconsistency}~\cite{inconsistency}, generating defects that do not align with the strict geometric constraints of rigid industrial parts. Unlike these fine-tuning approaches, our proposed method is \textit{optimization-based} (training-free), utilizing Null Embedding to preserve the original latent structure without the computational burden of training separate weights for every defect type.}

\textcolor{luckyred}{\textbf{Inpainting-based Methods.} Approaches like DefectFill~\cite{defectfill} and SDXL Inpainting~\cite{sdxl} utilize masks to generate defects within specific regions. Although they can generate realistic textures, these methods typically rely on \textit{user-provided masks}, which limits automation, or exhibit stochastic variance that unintentionally alters non-defective background regions. In contrast, our approach employs \textit{Text-Prompt augmentation} and \textit{RLI connections} to achieve precise, localized editing without requiring manual masks, offering a significant advantage in automated industrial pipelines.}

\textcolor{luckyred}{\textbf{Diffusion Inversion.} Standard inversion techniques, such as DDIM inversion~\cite{ddim}, are used to map real images back to the latent space. However, standard DDIM inversion often fails to reconstruct the fine details and complex textures inherent in industrial data. To address this, we adopt our \textit{Null Embedding Optimization} as a necessary technique to bridge this reconstruction gap, ensuring high-fidelity preservation of the original sample details.
}





\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/methodology1-c.png}
    \caption{The pipeline of our proposed method}
    \label{fig:methodology}
\end{figure}




% ===================================================
% Proposed Method
% ==================================================
\section{Proposed Method}
Figure~\ref{fig:methodology} shows the overall structure of the proposed approach, which utilizes the pre-trained SDXL model's capacity to produce highly realistic defect images. By capitalizing on pre-trained models, our method effectively approximates complex probability distributions that are essential for superior image synthesis. 

To enhance structural consistency and optimize latent representations, we introduce RLI connections in Section \ref{sec:rli_connections}. Recognizing that self-attention mechanisms inherently encode the spatial relationships and structural layout of an image, our approach integrates residual connections directly into these layers. Unlike conventional fine-tuning methods such as DreamBooth \cite{dreambooth} or LoRA \cite{lora}, this strategy explicitly preserves the structural backbone of the generated images while stabilizing the optimization process.

To further refine our process, we apply DDIM inversion~\cite{null} for rapid and precise extraction of latent values, which accelerates the image generation process. Detailed information can be found in Section~\ref{sec:ddim_inver}. Subsequently, to mitigate structural inconsistencies, we implement null embedding optimization. More details are available in Section~\ref{sec:NEP}. This technique improves our model’s ability to produce detailed images that closely mirror the original defect characteristics. Our scalable strategy also allows for fine control over the generated images, incorporating mechanisms such as a control step and the activation of attention maps~\cite{null} to diversify the defect generation process. More details are provided in Section~\ref{sec:defect-synthesis}.





\subsection{DDIM Inversion} 
\label{sec:ddim_inver}
\textcolor{luckyred}{We adopt the DDIM inversion process established by Song et al.~\cite{ddim} and Mokady et al.~\cite{null} to derive latent representations from images. This step serves as a critical deterministic prerequisite for the Null Embedding Optimization described in Section 3.3, ensuring that the generation process begins from a consistent noise state rather than random initialization.} DDIM~\cite{null} facilitates rapid sampling while preserving the core objectives of traditional Denoising Diffusion Probabilistic Models~\cite{ddpm} by utilizing a non-Markovian diffusion process. This is accomplished by deviating from the standard practice of introducing noise at every time step, resulting in deterministic output. The formulation is given by:
\begin{equation}
z_{t+1}=\sqrt{\bar\alpha_{t+1}}\underbrace{\left(\frac{z_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(z_t)}{\sqrt{\bar\alpha_t}}\right)}_{\text{predicted} \, z_t=f_\theta(z_t)}+\underbrace{\sqrt{1-\bar\alpha_{t+1}}\cdot\epsilon_\theta(z_t)}_{\text{direction pointing to} \, z_t}
\end{equation}

At the core of this process lies the extraction of the latent from the image, followed by the addition of noise. Here, $z_{t+1}$ represents the subsequent state, $z_t$ is the current state, and $\epsilon_{\theta}(z_t)$ denotes the noise estimated by the trained network. This noise prediction mechanism is fundamental for diffusion models, which are typically implemented using a U-Net architecture~\cite{sdxl}. The function
\[
f_{\theta}(z_t) = \frac{z_t - \sqrt{1 - \bar{\alpha}t} \cdot \epsilon_{\theta}(z_t)}{\sqrt{\bar{\alpha}t}}
\]
approximates the original clean data from the noisy state at time $t$. Moreover, the term $\sqrt{1 - \bar{\alpha}{t+1}} \cdot \epsilon_{\theta}(z_t)$ serves as the directional component that guides the latent back towards $z_t$, effectively steering the noise addition process. By adjusting the noise ratio at each time step through $\bar{\alpha}_t \coloneq \prod^t_{i=1}(1-\beta_i)$, with a schedule $\beta_0,\dots,\beta_T \in (0,1)$, this inversion function becomes critical for incrementally adding noise and finally obtaining the final latent state at time step $t$.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/methodology2-b.png}
    \caption{Residual Linear Interpolation~(RLI) Connections}
    \label{fig:methodology2}
\end{figure}


\subsection{Residual Linear Interpolation~(RLI) Connections}
\label{sec:rli_connections}
As shown in Figure~\ref{fig:methodology2}, to stabilize optimization and \textcolor{luckyred}{rigorously mitigate structural inconsistencies} in Stable Diffusion XL (SDXL) during null-text inversion, we introduce RLI connections. \textcolor{luckyred}{Unlike standard residual connections designed to facilitate gradient flow in deep networks, our RLI mechanism is specifically employed to preserve high-frequency structural information and constrain the optimization space against latent drift. This focus on structural fidelity parallels architectural refinements in AI-driven diagnostic systems, where preserving fine-grained details is similarly critical~\cite{comment3}.}

\textcolor{luckyred}{Building upon our comprehensive ablation studies on attention mechanisms, we refine the application of RLI to operate exclusively within the self-attention layers of the Up-blocks.} The mechanism modulates the attention output in a dual manner through a \textcolor{luckyred}{weighted interpolation mechanism}:
\begin{equation}
h_{\text{new}} = (1 - \alpha) \cdot h + \alpha \cdot r,
\end{equation}
where $h$ represents the post-attention hidden states, $r$ denotes the cached residual values, and $\alpha$ is an empirically tuned interpolation parameter. \textcolor{luckyred}{Unlike standard additive residuals ($y=x+f(x)$), this formulation allows us to explicitly control the trade-off between generative flexibility (from the model's diffusion process) and structural fidelity.}

\textcolor{luckyred}{Crucially, our method does not introduce modifications to the down or mid blocks. Our empirical analysis reveals that applying RLI to these components interferes with the model's semantic abstraction capabilities, yielding undesirable distortions or negligible improvements. By confining the intervention to the self-attention layers of the up blocks, our approach strikes a critical balance between representational flexibility and architectural stability, proving to be the necessary condition for optimizing high-resolution generative models.}

To determine optimal $\alpha$ values, we conduct extensive experiments and observe that setting $\alpha$ within $[0.1, 0.4]$ effectively stabilizes training while maintaining flexibility in image modifications. \textcolor{luckyred}{Our quantitative analysis confirms the efficacy of this selective module. As presented in Table~\ref{tab:main_gen_result}, comparing our proposed 'Up-block exclusive' configuration against the baseline 'Null-text inversion [28]' (w/o RLI) reveals a substantial performance gain of +1.45 dB in PSNR. This serves as an ablation study demonstrating that the targeted application of RLI is essential for maintaining high-fidelity industrial details that are otherwise lost during standard inversion.} Importantly, our approach eliminates the need for LoRA training, achieving high-quality image synthesis solely through the optimization process.




\subsection{Null Embedding Optimization}
\label{sec:NEP}
To address the structural inconsistencies commonly observed in generative models, we optimize the latents obtained during the reverse diffusion process along with the null embeddings at each time step. In typical DDIM~\cite{null} image generation, the latent representations extracted tend to deviate from the original image when combined with text embeddings. To resolve this, our method leverages negative embeddings as a null component, ensuring that the model preserves a complete latent representation at every stage. Specifically, we perform the optimization over 50 time steps. Conventional methods~\cite{null} do not yield satisfactory results for SDXL, as its architecture incorporates two encoders and two ClipTokenizers~\cite{clip}. Therefore, when using the U-Net architecture, both null group embedding and null pool embedding are employed. The objective of our optimization is to minimize the following loss function:
\begin{equation}
\min_{\emptyset_{t},\emptyset_{{pool}_{t}}} ||z_{t-1}^* - z_{t-1}(\bar{z}_t, \emptyset_{t},\emptyset_{pool_{t}}, C)||_2^2
\label{eq:minimization}
\end{equation}
In this equation, $\emptyset_{t}$ and $\emptyset_{{pool}_{t}}$ denote the \textcolor{luckyred}{learnable unconditional embeddings initialized from the empty prompt}, $z^*_{t-1}$ is the target latent \textcolor{luckyred}{derived from the inversion process}, and $z_{t-1}(\bar{z}_t, \emptyset_{t}, \emptyset_{{pool}_{t}}, C)$ represents the \textcolor{luckyred}{predicted latent state generated by the denoising function}, with $C$ being the conditional text embedding. The optimization process involves \textcolor{luckyred}{initializing $\emptyset_{t}$ and $\emptyset_{{pool}_{t}}$ from the empty text embedding}, computing the loss using Equation~\ref{eq:minimization}, calculating gradients with respect to the null embeddings and updating them via the Adam optimizer. These steps are iterated for a fixed number of iterations or until convergence is reached. Our experiments consistently demonstrate convergence with 50 time steps and 10 iterations per step, underscoring the efficiency and stability of our approach.

\subsection{Text Prompt-Based Defect Synthesis Optimization}
\label{sec:defect-synthesis}
We introduce a unified framework that leverages text prompt manipulation and parameter optimization to improve the diversity and precision of defect synthesis. Starting with a base prompt, \textit{photo of a 
defect image}, the method identifies key tokens—such as \textit{photo}, \textit{crack}, and \textit{defect}—from the activation maps of both SD and SDXL models. These tokens, which significantly influence the generated image, are then systematically modified using a blend-word technique; for example, building a corpus tailored to the defect domain, replacing \textit{crack} or \textit{defect} with \textit{dent} yields subtle yet effective changes in the defect morphology. 

The framework incorporates three crucial hyperparameters. The \textit{word amplifier (Eq)} modulates the attention weights of specific tokens to refine local feature intensity, the \textit{self step} determines the duration of preserving the original spatial structure via self-attention injection, and the \textit{cross step} regulates the fidelity to the original spatial context during the cross-attention injection. This integrated approach not only preserves the structural integrity of the original defect images but also generates a wide spectrum of variations, thereby enhancing the diversity of the data set. Ultimately, the proposed synthesis strategy contributes to improved performance in defect classification and segmentation tasks in industrial applications.


\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.0} % 행의 높이를 1.2배로 설정
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|c|c|c|c}
\toprule
Model & Image Size & PSNR$\uparrow$ & SSIM$\uparrow$ &LPIPS$\downarrow$\\ 
\midrule
Null-text inversion~\cite{null}& \multirow{3}{*}{512}   & 27.73 & 0.862 &0.069 \\
DISN~(w/ LoRA)~\cite{DISN}   &   & 28.67 & 0.875 &0.066\\ 
Ours~(w/o LoRA)\textcolor{luckyred}{(w/ RLI)}&  & \bfseries29.18& \bfseries0.884 & \bfseries0.063 \\
\midrule
ControlNet(Depth)~\cite{controlnet}  & \multirow{8}{*}{1024}  & 22.28& 0.721 & 0.190\\
CycleGAN~\cite{cyclegan} &   & 23.02 & 0.774 & 0.164 \\
SyNDGAN~\cite{eunhee} &   & 23.14 & 0.783 & 0.161\\
ControlNet(Canny)~\cite{controlnet}  &   & 23.86& 0.783 & 0.160\\
SDXL~(Inpaint)~\cite{sdxl}      &   & 26.79& 0.786 & 0.303\\
StyleGAN2-ADA~\cite{stylegan2ada}      &   & 29.16 & 0.873 & 0.226 \\
DISN~(w/ LoRA)\textcolor{luckyred}{(w/o RLI)}~\cite{DISN}   &   & 31.05 & 0.892 & 0.088 \\
Ours~(w/o LoRA)\textcolor{luckyred}{(w/ RLI)}   &   & \bfseries31.15 & 0.892 & \bfseries0.073 \\
    
\bottomrule
\end{tabular}
}
\caption{Quantitative comparison of image synthesis across diverse models and image size.}
\label{tab:main_gen_result}
\end{table}




\begin{figure}[]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/combined_by_psnr.png}
    \caption{Visual comparison of edge and pixel difference maps demonstrating the superior structural fidelity}
    \label{fig:structure_comparison}
\end{figure}
% ===================================================
% Experiments
% ===================================================

\section{Experimental Results}
\subsection{Optimization-Driven Defect Image Synthesis Preserving Structural Fidelity}
Table~\ref{tab:main_gen_result} presents a comprehensive quantitative comparison of our proposed method against several state-of-the-art models. At a resolution of 512$\times$512, our approach (Ours~(w/o LoRA)) achieves a PSNR of 29.18 dB, an SSIM of 0.884, and an LPIPS score of 0.063, thus surpassing both the null text inversion technique~\cite{null} and DISN~(w/ LoRA). At 1024$\times$1024 resolution, our method further improves the metrics, yielding a PSNR of 31.15 dB, an SSIM of 0.892, and an LPIPS score of 0.073. These results outperform those of ControlNet (Depth)~\cite{controlnet}, CycleGAN~\cite{cyclegan}, SyNDGAN~\cite{eunhee}, SDXL~(Inpaint)~\cite{sdxl}, StyleGAN2-ADA~\cite{stylegan2ada}, and even DISN~(w/ LoRA)(w/o RLI), demonstrating that our RLI-integrated optimization yields superior fidelity without the need for parameter fine-tuning.

\textcolor{luckyred}{To visually and scientifically substantiate that the structural fidelity is rigorously preserved, we conducted an additional analysis comparing the original and generated images using difference maps, as shown in Figure.~\ref{fig:structure_comparison}. Specifically, we visualized pixel-wise deviations using the Jet colormap and edge-based structural differences using the Hot colormap. As illustrated in the figure, baseline methods such as CycleGAN and SDXL~(Inpaint) exhibit significant `structural leakage', manifested as bright artifacts in the non-defective background regions. Crucially, a direct visual comparison with our preliminary work, DISN~(w/ LoRA), highlights the specific contribution of the RLI mechanism. While DISN demonstrates reasonable fidelity, its edge difference map still retains minor structural residuals along the component boundaries. In stark contrast, our method suppresses even these subtle deviations, maintaining near-zero edge deviation as represented by the dominant black regions. Furthermore, the Pixel Difference map confirms that our generative changes are surgically confined to the defect region (red circular markers), whereas other models inadvertently degrade the pristine background texture. This qualitative evidence confirms that the RLI module provides a more rigid structural constraint than parameter fine-tuning via LoRA, rigorously preserving the geometric integrity of industrial parts.}

\textcolor{luckyred}{Finally, it is crucial to address the statistical nature and efficiency of our framework. Unlike standard stochastic generative models that require significance testing to account for random variance, our method employs DDIM inversion, which is a deterministic process. This ensures that the synthesis pipeline yields consistent, reproducible outputs for a given input and prompt, rendering confidence intervals for stochastic variance inapplicable in this context. Moreover, a critical advantage of our framework is that it eliminates the need for LoRA training. Using an optimization-based approach augmented only with RLI connections, our method can directly refine the latent representations of the SDXL model. This strategy effectively prevents structural distortions without necessitating additional training steps, thereby streamlining the synthesis process while maintaining high fidelity.}


\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/attention_map-b.png}
    \caption{Visual comparison of \textit{token-wise attention maps} across models: Stable Diffusion (a), Stable Diffusion XL (b), SDXL with LoRA (c), and our proposed method (d).}
    \label{fig:attention_map}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/aug_defect-b.png}
    \caption{Various defect images are generated by modifying prompts using word pairs like \textit{crack} with \textit{blistering}, \textit{crack} with \textit{dent}, \textit{defect} with \textit{rust}, \textit{photo} with \textit{corrosion}, \textit{defect} with \textit{peeling} and \textit{crack} with \textit{scratch}.}
    \label{fig:aug_defect}
\end{figure}


\subsection{Diverse and Precise Defect Synthesis}
We present a unified framework to address structural inconsistencies and enhance defect diversity through text prompt manipulation and parameter optimization. In our approach, both SD and SDXL models utilize two types of embeddings. However, our method uses one embedding to generate diverse defect patterns. The base prompt, \textit{photo of a crack defect image}, serves as a starting point and can be systematically modified by substituting key terms to alter the appearance of the defect.

As shown in Figure~\ref{fig:attention_map}, we extract the most representative defects-related tokens from the activation maps, identifying \textit{photo}, \textit{crack}, and \textit{defect} as the dominant contributors. \textcolor{luckyred}{The term `crack' was selected as the representative defect token because cracks often represent the most challenging type of structural defect in industrial vision. Cracks are characteristically thin, exhibit low contrast, and possess complex, non-Gaussian distributions, making them highly sensitive to the structural inconsistencies of generative models. Analyzing the attention maps for `crack' therefore serves as a rigorous visual test case to confirm that our Null Embedding Optimization successfully confines defect modification without causing 'structural leakage' in the pristine background. This visualization demonstrates how each word contributes to image interpretation, emphasizing the value of individual token analysis in understanding visual defects. Notably, while attention maps naturally become more defined with larger models and the use of LoRA as seen in (b) and (c), our new approach (d) achieves comparably sharp and refined attention maps without relying on LoRA, underscoring the effectiveness of our improved attention mechanism.} We then experiment with various substitutions for these tokens. For example, by replacing \textit{crack} (or \textit{defect}) with \textit{dent} using a mixing word technique, we successfully modify the morphology of the generated defect, as illustrated in Figure~\ref{fig:aug_defect}. In this context, introduce three crucial hyperparameters: the \textit{word amplifier}, which modulates the text guidance; the \textit{replace step}, which governs the number of diffusion iterations during inversion; and the newly introduced \textit{cross step}, which controls the degree of modification applied to the text prompt during substitution.

Guided by defect typologies prevalent in industrial environments, our unified framework seamlessly integrates text-driven modifications with rigorous parameter optimization. The result is a robust and flexible synthesis strategy that generates a wide spectrum of defect images with enhanced fidelity and diversity, thereby improving the practical applicability of our approach in industrial defect detection systems.







\subsection{Comprehensive Dataset Collection and Synthetic Data Augmentation}
\textcolor{luckyred}{In this work, to rigorously evaluate robustness, we employ three distinct datasets ranging from a proprietary industrial dataset to standard public benchmarks. First, the Carbide Insert Dataset (In-House) utilizes authentic images of cemented carbide inserts. As detailed in Table~\ref{tab:Carbide Insert,Training and testing dataset for classification.}, the classification subset consists of high-quality bright field images selected from four lighting conditions~\cite{eunhee}. For segmentation (Table~\ref{tab:Carbide Insert, Training and testing dataset for segmentation.}), the dataset presents an extreme class imbalance with a defect ratio of only 0.23\% (Normal: 99.77\%). This poses a significantly harder challenge due to the absolute scarcity of defect pixels compared to public benchmarks, prompting us to double the defect samples to mitigate bias toward the background.}

\textcolor{luckyred}{To validate our method on standard benchmarks, we employ the Kolektor Surface Defect Dataset (KolektorSDD) and the MVTec AD Dataset. As shown in Table~\ref{tab:KolektorSDD, Training and testing dataset for segmentation.}, while KolektorSDD exhibits area imbalance with microscopic defects, its class imbalance (12\% defect ratio) is moderate compared to our In-House dataset (0.23\% vs. 12\%). Similarly, for the MVTec AD dataset covering 15 categories from textures to rigid objects, defect ratios generally range from 10--25\% (e.g., 0.10 for Grid, 0.26 for Pill) as detailed in Table~\ref{tab:MvTec AD, Training and testing dataset for segmentation.}, contrasting with the extreme scarcity of our In-House data. We applied prompt-to-prompt augmentation to diversify morphologies for each category.} 

\textcolor{luckyred}{Across all three datasets, synthetic data augmentation was performed using our unified framework integrating RLI connections~(Section~\ref{sec:rli_connections}) (with $\alpha=0.1$) and Null Embedding Optimization. The prompt-to-prompt augmentation~(Section~\ref{sec:defect-synthesis}) parameters were consistently set (Cross step = 0.8, Self step = 0.6, Eq = 2) to ensure fair and consistent evaluation of the generated structural fidelity and diversity across the varying levels of data imbalance.}

\begin{table}[!t]
\centering
\renewcommand{\baselinestretch}{1.0}
\renewcommand{\arraystretch}{0.6}

% 가운데 정렬되는 가변 너비 컬럼 타입 'Y' 정의 (이미 정의되어 있다면 생략 가능)
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% tabularx 사용: 전체 너비 맞춤 + 데이터 열(Y) 가운데 정렬 + 세로선 제거
\begin{tabularx}{\textwidth}{l|Y|Y|Y|Y} \toprule % 상단 굵은 줄
Dataset & Normal & Defect & Total & Ratio \\ \midrule % 중간 얇은 줄
Original (\textit{{$D_co$}}) & 236 & 23 & 259 & 1:10 \\ 
Synthetic1 (\textit{{$D_{cS1}$}}) & 236 & 46 & 282 & 1:5 \\ 
Test (\textit{{$D_c{test}$}}) & 58 & 39 & 97 & 1:1.5 \\ 
\bottomrule % 하단 굵은 줄
\end{tabularx}
\caption{Carbide Insert,Training and testing dataset for classification.}
\label{tab:Carbide Insert,Training and testing dataset for classification.}
\end{table}

\begin{table}[!t]
\centering
\renewcommand{\baselinestretch}{1.0}
\renewcommand{\arraystretch}{0.6}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\multirow{2}{*}{Dataset}
& \multirow{2}{*}{Normal}
& \multirow{2}{*}{Defect}
& \multirow{2}{*}{Total}
& \multicolumn{3}{c}{Ratio (\%)}  \\ \cmidrule(lr){5-7}  % (%) 제거
& & & & \multicolumn{1}{c}{Normal} & \multicolumn{1}{c}{\textit{Attached}} & \textit{Broken} \\ \midrule


Original (\textit{{$D_so$}})
& 7 & 210 & 210 & \multicolumn{1}{c}{0.9969} & \multicolumn{1}{c}{0.0022} & 0.0009 \\ % 합: 1.0000

Synthetic (\textit{{$D_{sS1}$}})
& 7 & 210 & 420 & \multicolumn{1}{c}{0.9966} & \multicolumn{1}{c}{0.0022} & 0.0012 \\ % 합: 1.0000

Test (\textit{{$D_s{test}$}})
& 1 & 59 & 59 & \multicolumn{1}{c}{0.9966} & \multicolumn{1}{c}{0.0021} & 0.0013 \\ % 합: 1.0000




\bottomrule
\end{tabular}%
}
\caption{Carbide Insert, Training and testing dataset for segmentation.}
\label{tab:Carbide Insert, Training and testing dataset for segmentation.}
\end{table}

% ===================================================



\begin{table}[!t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\toprule
\multicolumn{2}{c|}{\textbf{Category}} & \textbf{Bottle} & \textbf{Cable} & \textbf{Capsule} & \textbf{Carpet} & \textbf{Grid} & \textbf{Hazelnut} & \textbf{Leather} & \textbf{Metal} & \textbf{Pill} & \textbf{Screw} & \textbf{Tile} & \textbf{Toothbrush} & \textbf{Transistor} & \textbf{Wood} & \textbf{Zipper} \\
\midrule
\multirow{5}{*}{\textbf{Original}} 
& Normal & 177 & 177 & 163 & 233 & 234 & 355 & 197 & 172 & 194 & 259 & 187 & 45 & 193 & 216 & 179 \\
& Defect & 31 & 45 & 53 & 42 & 27 & 34 & 44 & 45 & 68 & 58 & 41 & 15 & 20 & 29 & 58 \\
& Total & 208 & 222 & 216 & 275 & 261 & 389 & 241 & 217 & 262 & 317 & 228 & 60 & 213 & 245 & 237 \\
& Ratio(N) & 0.85 & 0.80 & 0.75 & 0.85 & 0.90 & 0.91 & 0.82 & 0.79 & 0.74 & 0.82 & 0.82 & 0.75 & 0.91 & 0.88 & 0.76 \\
& Ratio(D) & 0.15 & 0.20 & 0.25 & 0.15 & 0.10 & 0.09 & 0.18 & 0.21 & 0.26 & 0.18 & 0.18 & 0.25 & 0.09 & 0.12 & 0.24 \\
\midrule
\multirow{5}{*}{\textbf{Synthetic}} 
& Normal & 177 & 177 & 163 & 233 & 234 & 355 & 197 & 172 & 194 & 259 & 187 & 45 & 193 & 216 & 179 \\
& Defect & 62 & 90 & 106 & 84 & 54 & 68 & 88 & 90 & 136 & 116 & 82 & 30 & 40 & 58 & 116 \\
& Total & 239 & 267 & 269 & 317 & 288 & 423 & 285 & 262 & 330 & 375 & 269 & 75 & 233 & 274 & 295 \\
& Ratio(N) & 0.74 & 0.66 & 0.61 & 0.74 & 0.81 & 0.84 & 0.69 & 0.66 & 0.59 & 0.69 & 0.70 & 0.60 & 0.83 & 0.79 & 0.61 \\
& Ratio(D) & 0.26 & 0.34 & 0.39 & 0.26 & 0.19 & 0.16 & 0.31 & 0.34 & 0.41 & 0.31 & 0.30 & 0.40 & 0.17 & 0.21 & 0.39 \\
\midrule
\multirow{5}{*}{\textbf{Test}} 
& Normal & 34 & 34 & 61 & 48 & 22 & 36 & 49 & 54 & 78 & 69 & 42 & 18 & 12 & 24 & 60 \\
& Defect & 23 & 23 & 41 & 32 & 15 & 24 & 33 & 36 & 52 & 46 & 28 & 12 & 8 & 16 & 40 \\
& Total & 57 & 57 & 102 & 80 & 37 & 60 & 82 & 90 & 130 & 115 & 70 & 30 & 20 & 40 & 100 \\
& Ratio(N) & 0.60 & 0.60 & 0.60 & 0.60 & 0.59 & 0.60 & 0.60 & 0.60 & 0.60 & 0.60 & 0.60 & 0.60 & 0.60 & 0.60 & 0.60 \\
& Ratio(D) & 0.40 & 0.40 & 0.40 & 0.40 & 0.41 & 0.40 & 0.40 & 0.40 & 0.40 & 0.40 & 0.40 & 0.40 & 0.40 & 0.40 & 0.40 \\
\bottomrule
\end{tabular}
}
\caption{MvTec AD, Training and testing dataset for segmentation.}
\label{tab:MvTec AD, Training and testing dataset for segmentation.}
\end{table}


\begin{table}[!t]
\centering
\renewcommand{\baselinestretch}{1.2}
\renewcommand{\arraystretch}{0.6}

% 1. 가운데 정렬되면서 너비를 꽉 채우는 새로운 컬럼 타입 'Y' 정의
\newcolumntype{Y}{>{\centering\arraybackslash}X}

% 2. tabularx 환경 사용 (너비는 \textwidth)
% 첫 번째 열은 왼쪽 정렬(l), 나머지는 위에서 정의한 Y(가운데 정렬+자동 너비) 사용
\begin{tabularx}{\textwidth}{l|Y|Y|Y|Y|Y} 
\toprule
\multirow{2}{*}{Dataset}
& \multirow{2}{*}{Normal}
& \multirow{2}{*}{Defect}
& \multirow{2}{*}{Total}
& \multicolumn{2}{c}{Ratio} \\ \cmidrule(lr){5-6}
& & & & Normal & Defect \\ \midrule



Original
& 247 & 34 & 281 & 0.88 & 0.12 \\

Synthetic
& 247 & 68 & 315 & 0.78 & 0.22 \\ 

Test
& 52 & 8 & 60 & 0.87 & 0.13 \\
\bottomrule
\end{tabularx}
\caption{KolektorSDD, Training and testing dataset for segmentation.}
\label{tab:KolektorSDD, Training and testing dataset for segmentation.}
\end{table}


%%%%%%%%%% new table %%%%%%%%%%
\begin{table}[!t]
    \centering
    % \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{cc}
        % 왼쪽 열: 첫 번째 절반 (예: Densenet121, ResNet18, ResNet50)
        \begin{minipage}{0.475\textwidth}
            \centering
            \resizebox{\textwidth}{!}{%
            \begin{tabular}{l|c|c|c}
            \toprule
            Model & Method &Augmentation & Accuracy \\
            \midrule
                \multirow{10}{*}{Densenet121~\cite{densnet}}
                & \multirow{3}{*}{Traditional} 
                  & MixUp~(\textit{{$D_co$}})    & 68.04 \\
                  && CutMix~(\textit{{$D_co$}})   & 68.04 \\
                  && original~(\textit{{$D_co$}}) & \textbf{75.26} \\
                \cmidrule(lr){2-4}
                & \multirow{7}{*}{Proposed} 
                  & syn~(\textit{{$D_{cS1}^{syn}$}})     & 86.60 \\
                  && bilstering~(\textit{{$D_{cS1}^{mod}$}}) & 85.57 \\
                  && rust~(\textit{{$D_{cS1}^{mod}$}}) & 84.54 \\
                  && degradation~(\textit{{$D_{cS1}^{mod}$}}) & 86.60 \\
                  && corrosion~(\textit{{$D_{cS1}^{mod}$}}) & 86.60 \\
                  && wear~(\textit{{$D_{cS1}^{mod}$}}) & 89.69 \\
                  && peeling~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{91.75} \\
                \midrule
                \multirow{10}{*}{ResNet18~\cite{resnet}} 
                & \multirow{3}{*}{Traditional} 
                  & CutMix~(\textit{{$D_co$}})   & 65.98 \\
                  && MixUp~(\textit{{$D_co$}})    & 71.13 \\
                  && original~(\textit{{$D_co$}}) & \textbf{73.20} \\
                \cmidrule(lr){2-4}
                & \multirow{7}{*}{Proposed} 
                  & syn~(\textit{{$D_{cS1}^{syn}$}})& 82.47 \\
                  && bilstering~(\textit{{$D_{cS1}^{mod}$}}) & 76.29 \\
                  && rust~(\textit{{$D_{cS1}^{mod}$}}) & 81.44 \\
                  && wear~(\textit{{$D_{cS1}^{mod}$}}) & 81.44 \\
                  && peeling~(\textit{{$D_{cS1}^{mod}$}}) & 84.54 \\
                  && corrosion~(\textit{{$D_{cS1}^{mod}$}}) & 85.57 \\
                  && degradation~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{86.60} \\
                \midrule
                \multirow{10}{*}{ResNet50~\cite{resnet}}
                & \multirow{3}{*}{Traditional} 
                  & MixUp~(\textit{{$D_co$}})    & 65.98 \\
                  && CutMix~(\textit{{$D_co$}})   & 68.04 \\
                  && original~(\textit{{$D_co$}}) & \textbf{69.07} \\
                \cmidrule(lr){2-4}
                & \multirow{7}{*}{Proposed} 
                  & syn~(\textit{{$D_{cS1}^{syn}$}})  & 73.20 \\
                  && peeling~(\textit{{$D_{cS1}^{mod}$}}) & 74.23 \\
                  && degradation~(\textit{{$D_{cS1}^{mod}$}}) &74.23 \\
                  && rust~(\textit{{$D_{cS1}^{mod}$}}) & 75.26 \\
                  && bilstering~(\textit{{$D_{cS1}^{mod}$}}) & 76.29 \\
                  && wear~(\textit{{$D_{cS1}^{mod}$}}) & 78.35 \\
                  && corrosion~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{78.35} \\
                \bottomrule
                \end{tabular}
            }
        \end{minipage}
        &
        \begin{minipage}{0.51\textwidth}
            \centering
            \resizebox{\textwidth}{!}{%
            \begin{tabular}{l|c|c|c}
                \toprule
                Model &Method&Augmentation & Accuracy \\
                \midrule
                \multirow{10}{*}{ResNet101~\cite{resnet}}
                & \multirow{3}{*}{Traditional} 
                  & CutMix~(\textit{{$D_co$}})   & 68.04 \\
                  && MixUp~(\textit{{$D_co$}})    & 69.07 \\
                  && original~(\textit{{$D_co$}}) & \textbf{70.10} \\
                \cmidrule(lr){2-4}
                & \multirow{7}{*}{Proposed} 
                  & syn~(\textit{{$D_{cS1}^{syn}$}}) & 74.23 \\
                  && wear~(\textit{{$D_{cS1}^{mod}$}}) & 74.23 \\
                  && corrosion~(\textit{{$D_{cS1}^{mod}$}}) & 75.26 \\
                  && degradation~(\textit{{$D_{cS1}^{mod}$}})& 75.26 \\
                  && peeling~(\textit{{$D_{cS1}^{mod}$}}) & 77.32 \\
                  && rust~(\textit{{$D_{cS1}^{mod}$}}) & 78.35 \\
                  && bilstering~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{79.38} \\
                \midrule
                \multirow{10}{*}{EfficientNetV2-M~\cite{EfficientNetV2}}
                & \multirow{3}{*}{Traditional} 
                  & MixUp~(\textit{{$D_co$}})    & 83.51 \\
                  && CutMix~(\textit{{$D_co$}})   & 84.54 \\
                  && original~(\textit{{$D_co$}}) & \textbf{89.69} \\
                \cmidrule(lr){2-4}
                & \multirow{7}{*}{Proposed} 
                  & syn~(\textit{{$D_{cS1}^{syn}$}})     & 92.78 \\
                  && wear~(\textit{{$D_{cS1}^{mod}$}}) & 92.78 \\
                  && bilstering~(\textit{{$D_{cS1}^{mod}$}}) & 93.81 \\
                  && peeling~(\textit{{$D_{cS1}^{mod}$}}) & 93.81 \\
                  && rust~(\textit{{$D_{cS1}^{mod}$}}) & 94.85 \\
                  && degradation~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{95.88} \\
                  && corrosion~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{95.88} \\
                \midrule
                \multirow{10}{*}{EfficientNetV2-L~\cite{EfficientNetV2}}
                & \multirow{3}{*}{Traditional} 
                  & MixUp~(\textit{{$D_co$}})    & 82.47 \\
                  && CutMix~(\textit{{$D_co$}})   & 86.60 \\
                  && original~(\textit{{$D_co$}}) & \textbf{91.75} \\
                \cmidrule(lr){2-4}
                & \multirow{7}{*}{Proposed} 
                  & syn~(\textit{{$D_{cS1}^{syn}$}})     & 93.81 \\
                  && degradation~(\textit{{$D_{cS1}^{mod}$}}) & 94.85 \\
                  && wear~(\textit{{$D_{cS1}^{mod}$}}) & 94.85 \\
                  && bilstering~(\textit{{$D_{cS1}^{mod}$}}) & 94.85 \\
                  && corrosion~(\textit{{$D_{cS1}^{mod}$}}) & 94.85 \\
                  && rust~(\textit{{$D_{cS1}^{mod}$}}) & 95.88 \\
                  && peeling~(\textit{{$D_{cS1}^{mod}$}}) & \textbfc{red}{96.91} \\
                \bottomrule
                \end{tabular}
            }
        \end{minipage}
    \end{tabular}
\caption{Classification performance comparison on the Carbide Insert Dataset.}

    \label{tab:classification_result}
\end{table}

\subsection{Defect Classification: Implementation and Performance}
Table~\ref{tab:classification_result} presents the results of our defect classification experiments on various CNN-based models. Our approach leverages a two-tier synthetic data generation strategy. Specifically, we generate two types of synthetic images. The first subset, denoted as \(D_{cS1}^{syn}\), is produced by closely following the original image, producing synthetic images labeled as \textit{syn} that mimic the original defect distribution. In contrast, the second subset, denoted as \(D_{cS1}^{mod}\), is generated by modifying the key tokens in the text prompt to create additional defect variants (\textit{e.g.}, \textit{corrosion} and \textit{degradation}), thereby expanding the diversity of the synthetic dataset.
% Table~\ref{tab:classification_result} presents the results of our defect classification experiments across various CNN-based models. Our approach leverages a two-tier synthetic data generation strategy. The dataset $D_{cS1}^{mod}$ is produced by closely following the original image, yielding synthetic images labeled as \textit{syn} that mimic the original defect distribution. In contrast, $D_{cS1}^{mod}$ is generated by modifying key tokens in the text prompt to create additional defect variants (\textit{e.g.} \textit{corrosion} and \textit{degradation}), thereby expanding the diversity of the synthetic dataset.

Incorporating these synthetic datasets significantly improves classification performance. For example, DenseNet121's test accuracy increase from 75.26\% with the original data to \textbf{91.75\%} using our method. Similarly, ResNet18 improves from 73.20\% to \textbf{86.60\%}, ResNet50 from 69.07\% to \textbf{78.35\%}, ResNet101 from 70.10\% to \textbf{79.38\%}, and both EfficientNetV2-M and V2-L exhibit significant gains. In particular, our method consistently outperforms traditional augmentation techniques such as MixUp~\cite{MixUp} and CutMix~\cite{CutMix}. Previous research reports that the limited amount of defect data and class imbalance constrain the performance of deep learning models for classification and segmentation~\cite{dataaug1,dataaug2,dataaug3}. In particular, conventional data augmentation methods, for example, CutMix and Mixup, are noted to fail in preserving the structural features of defects and tend to blur the boundaries between classes (for classification) or defect regions (for segmentation), leading to performance degradation. Furthermore, traditional data augmentation does not enhance the inherent diversity of defects, and in environments with scarce defect samples, approaches to synthesize new defects are proposed. Our experiments using CutMix and Mixup confirm that the performance for both classification and segmentation is not improved, which is consistent with previous findings. We demonstrate that our prompt modification strategy not only produces synthetic data closely aligned with the original distribution but also enables the creation of new, high-performing defect categories.


\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.0} % 행의 높이를 1.2배로 설정
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Model & \begin{tabular}[c]{@{}c@{}}Trained\\ on\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{Attached}\\ IoU$\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{Attached}\\ Acc$\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{Broken}\\ IoU$\uparrow$\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{Broken}\\ Acc$\uparrow$\end{tabular} & mIoU & mAcc \\ \midrule
\multirow{3}{*}{Fast-SCNN} & Original dataset~(\textit{{$D_s{o}$}})   & 30.04 & 34.39 & 22.21 & 37.02 & 26.125  & 35.705  \\  
                         & w/o re-labeling   & 35.89 & 44.42 & 22.25 & \textbf{39.79} & 29.07  & 42.105  \\  
                         & w/ re-labeling   & \textbf{36.94} & \textbf{78.25} & \textbf{23.58} & 38.8  & \textbf{30.26} & \textbf{58.525} \\ \midrule
\multirow{3}{*}{MobileNetV3} & Original dataset~(\textit{{$D_s{o}$}})   & 39.62 & 44.95 & 38.11 &\textbf{72.22} & 38.865 & 58.585  \\  
                         & w/o re-labeling   & 42.65 & 47.27 & 37.72 & 62.55 & 40.185 & 54.91   \\  
                         & w/ re-labeling   & \textbf{53.84} & \textbf{79.72} & \textbf{39.96} & 56.68 & \textbf{46.9}  & \textbf{68.2}   \\ \bottomrule
\end{tabular}%
}
\caption{Impact of re-labeling on defect segmentation performance.}
\label{tab:comparison_re-labeling}
\end{table}


\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.0} % 행의 높이를 1.5배로 설정
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
Dataset        
& Loss Func.        
& \begin{tabular}[c]{@{}c@{}}\textit{Attached}\\ IoU$\uparrow$\end{tabular} 
& \begin{tabular}[c]{@{}c@{}}\textit{Attached}\\ Acc$\uparrow$\end{tabular} 
& \begin{tabular}[c]{@{}c@{}}\textit{Broken}\\ IoU$\uparrow$\end{tabular} 
& \begin{tabular}[c]{@{}c@{}}\textit{Broken}\\ Acc$\uparrow$\end{tabular} 
& mIoU   
& mAcc   \\ \midrule
\multirow{4}{*}{Original}
& Focal Loss& \textbf{44.34}& \ul{61.05}& \textbf{22.33}& \ul{36.31}& \textbf{33.335}& \ul{48.68}\\ 
& Tversky Loss& 41.29& 57.77& 19.25& 26.29& 30.27& 42.03\\ \
& CrossEntropy Loss& 37.53& 41.94& 9.85& 10.37& 23.69& 26.155\\ 
& Dice Loss& 37.17& 52.99& 7.39& 7.6& 22.28&30.295\\
& Focal + Tversky Loss& 24.11& \textbf{77.2}& 21.31& \textbf{42.35}& 22.71&\textbf{59.775}\\ 
& Dice + CrossEntropy Loss& 25.6& 27.72& 12.89& 14.27& 19.245&20.995\\ 
\bottomrule
\end{tabular}%
}
\caption{Comparison of segmentation performance using different loss functions. Focal Loss achieves the highest mIoU and second position on mAcc.}
\label{tab:comparison_loss}
\end{table}







\begin{table}[]
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Model}
& \begin{tabular}[c]{@{}c@{}}\textbf{Att. IoU}\end{tabular}
& \begin{tabular}[c]{@{}c@{}}\textbf{Bro. IoU}\end{tabular}
& \begin{tabular}[c]{@{}c@{}}\textbf{mIoU}\end{tabular}
& \begin{tabular}[c]{@{}c@{}}\textbf{mAcc}\end{tabular} \\
\cmidrule(lr){2-5}
& \multicolumn{4}{c}{\textit{Format: Original / Augmented (+Gain) or (-Loss)}} \\
\midrule
% mIoU Gain: +1.74 (ViT) - 1등
\textbf{ViT} & 30.59 / 33.68 \color{red}{\textbf{(+3.09)}} & 19.00 / 19.40 \color{red}{\textbf{(+0.40)}} & 24.80 / 26.54 \color{red}{\textbf{(+1.74)}} & 32.88 / 35.03 \color{red}{\textbf{(+2.15)}} \\
% mIoU Gain: +2.12 (DANet) - 2등
\textbf{DANet} & 46.89 / 50.41 \color{red}{\textbf{(+3.52)}} & 26.38 / 27.09 \color{red}{\textbf{(+0.71)}} & 36.64 / 38.75 \color{red}{\textbf{(+2.12)}} & 68.89 / 59.19 \color{blue}{\textbf{(-9.70)}} \\
% mIoU Gain: +2.71 (CCNet) - 3등
\textbf{CCNet} & 37.56 / 41.63 \color{red}{\textbf{(+4.07)}} & 25.73 / 27.07 \color{red}{\textbf{(+1.34)}} & 31.65 / 34.35 \color{red}{\textbf{(+2.71)}} & 70.68 / 59.05 \color{blue}{\textbf{(-11.63)}} \\
% mIoU Gain: +3.19 (BEiT) - 4등
\textbf{BEiT} & 29.00 / 34.59 \color{red}{\textbf{(+5.59)}} & 18.76 / 19.55 \color{red}{\textbf{(+0.79)}} & 23.88 / 27.07 \color{red}{\textbf{(+3.19)}} & 35.28 / 42.57 \color{red}{\textbf{(+7.29)}} \\
% mIoU Gain: +3.41 (ANN) - 5등
\textbf{ANN} & 42.04 / 48.78 \color{red}{\textbf{(+6.74)}} & 25.45 / 25.53 \color{red}{\textbf{(+0.08)}} & 33.75 / 37.16 \color{red}{\textbf{(+3.41)}} & 59.67 / 65.26 \color{red}{\textbf{(+5.59)}} \\
% mIoU Gain: +3.89 (PSPNet) - 6등
\textbf{PSPNet} & 37.85 / 45.25 \color{red}{\textbf{(+7.40)}} & 27.19 / 27.56 \color{red}{\textbf{(+0.37)}} & 32.52 / 36.41 \color{red}{\textbf{(+3.89)}} & 50.83 / 62.27 \color{red}{\textbf{(+11.44)}} \\
% mIoU Gain: +4.13 (Fast-SCNN) - 7등
\textbf{Fast-SCNN} & 30.04 / 36.94 \color{red}{\textbf{(+6.90)}} & 22.21 / 23.58 \color{red}{\textbf{(+1.37)}} & 26.13 / 30.26 \color{red}{\textbf{(+4.13)}} & 35.71 / 58.53 \color{red}{\textbf{(+22.82)}} \\
% mIoU Gain: +4.72 (GCNet) - 8등
\textbf{GCNet} & 41.87 / 49.97 \color{red}{\textbf{(+8.10)}} & 26.24 / 27.58 \color{red}{\textbf{(+1.34)}} & 34.06 / 38.78 \color{red}{\textbf{(+4.72)}} & 56.45 / 65.43 \color{red}{\textbf{(+8.99)}} \\
% mIoU Gain: +6.57 (MobileNetV3) - 9등
\textbf{MobileNetV3} & 40.51 / 51.58 \color{red}{\textbf{(+11.07)}} & 37.00 / 39.07 \color{red}{\textbf{(+2.07)}} & 38.76 / 45.33 \color{red}{\textbf{(+6.57)}} & 59.36 / 69.77 \color{red}{\textbf{(+10.41)}} \\
% mIoU Gain: +8.39 (APCNet) - 10등
\textbf{APCNet} & 43.39 / 48.39 \color{red}{\textbf{(+5.00)}} & 16.42 / 28.20 \color{red}{\textbf{(+11.78)}} & 29.91 / 38.30 \color{red}{\textbf{(+8.39)}} & 34.09 / 62.51 \color{red}{\textbf{(+28.43)}} \\
% mIoU Gain: +8.67 (BiSeNetV2) - 11등
\textbf{BiSeNetV2} & 25.31 / 42.12 \color{red}{\textbf{(+16.81)}} & 25.82 / 26.35 \color{red}{\textbf{(+0.53)}} & 25.57 / 34.24 \color{red}{\textbf{(+8.67)}} & 42.06 / 54.35 \color{red}{\textbf{(+12.29)}} \\
% mIoU Gain: +9.80 (UPerNet) - 12등
\textbf{UPerNet} & 34.44 / 54.10 \color{red}{\textbf{(+19.66)}} & 36.01 / 35.96 \color{blue}{\textbf{(-0.05)}} & 35.23 / 45.03 \color{red}{\textbf{(+9.80)}} & 55.94 / 72.03 \color{red}{\textbf{(+16.09)}} \\
\bottomrule
\end{tabular}%
}
\caption{Segmentation performance comparison on the \textcolor{luckyred}{Carbide Insert Dataset}, where synthetic data is generated specifically using the `\textit{peeling}' prompt modification. Abbreviations: Att. and Bro. refer to the \textit{Attached} and \textit{Broken} defect categories, respectively.}
\label{tab:carbie_segmentation_result}
\end{table}




\begin{table*}[]
\centering
\resizebox{1.0\textwidth}{!}{%
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{l|c|cccccc}
\multicolumn{8}{c}{\textbf{MvTec AD Part 1 / 3}} \\
\toprule
\textbf{Category} & \textbf{Baseline} & \textbf{Blistering} & \textbf{Corrosion} & \textbf{Dent} & \textbf{Peeling} & \textbf{Rust} & \textbf{Scratch} \\
\midrule
\multicolumn{8}{c}{\textbf{Model: Fast-SCNN}} \\
\midrule
Bottle & 69.60 & 72.82 \color{red}{(+3.22)} & 74.65 \color{red}{(+5.05)} & 74.37 \color{red}{(+4.77)} & 73.25 \color{red}{(+3.65)} & 74.68 \color{red}{(+5.08)} & 73.62 \color{red}{(+4.02)} \\
Cable & 49.93 & 62.14 \color{red}{(+12.21)} & 58.56 \color{red}{(+8.63)} & 61.50 \color{red}{(+11.57)} & 59.88 \color{red}{(+9.95)} & 59.85 \color{red}{(+9.92)} & 58.27 \color{red}{(+8.34)} \\
Capsule & 25.27 & 26.56 \color{red}{(+1.29)} & 31.41 \color{red}{(+6.14)} & 35.71 \color{red}{(+10.44)} & 31.33 \color{red}{(+6.06)} & 28.43 \color{red}{(+3.16)} & 29.31 \color{red}{(+4.04)} \\
Carpet & 59.26 & 59.37 \color{red}{(+0.11)} & 61.81 \color{red}{(+2.55)} & 60.38 \color{red}{(+1.12)} & 59.55 \color{red}{(+0.29)} & 59.47 \color{red}{(+0.21)} & 60.47 \color{red}{(+1.21)} \\
Grid & 30.66 & 41.98 \color{red}{(+11.32)} & 39.76 \color{red}{(+9.10)} & 33.06 \color{red}{(+2.40)} & 40.47 \color{red}{(+9.81)} & 37.56 \color{red}{(+6.90)} & 44.48 \color{red}{(+13.82)} \\
Hazelnut & 72.10 & 72.64 \color{red}{(+0.54)} & 76.07 \color{red}{(+3.97)} & 74.63 \color{red}{(+2.53)} & 73.01 \color{red}{(+0.91)} & 74.01 \color{red}{(+1.91)} & 75.10 \color{red}{(+3.00)} \\
Leather & 59.79 & 60.43 \color{red}{(+0.64)} & 64.06 \color{red}{(+4.27)} & 60.72 \color{red}{(+0.93)} & 61.02 \color{red}{(+1.23)} & 62.45 \color{red}{(+2.66)} & 62.05 \color{red}{(+2.26)} \\
Metal Nut & 85.97 & 87.73 \color{red}{(+1.76)} & 87.37 \color{red}{(+1.40)} & 87.37 \color{red}{(+1.40)} & 87.39 \color{red}{(+1.42)} & 87.86 \color{red}{(+1.89)} & 87.28 \color{red}{(+1.31)} \\
Pill & 72.57 & 74.12 \color{red}{(+1.55)} & 75.61 \color{red}{(+3.04)} & 75.05 \color{red}{(+2.48)} & 75.03 \color{red}{(+2.46)} & 75.65 \color{red}{(+3.08)} & 74.64 \color{red}{(+2.07)} \\
Screw & 25.68 & 31.17 \color{red}{(+5.49)} & 31.15 \color{red}{(+5.47)} & 27.91 \color{red}{(+2.23)} & 26.51 \color{red}{(+0.83)} & 32.52 \color{red}{(+6.84)} & 31.10 \color{red}{(+5.42)} \\
Tile & 80.45 & 81.05 \color{red}{(+0.60)} & 80.87 \color{red}{(+0.42)} & 81.89 \color{red}{(+1.44)} & 80.81 \color{red}{(+0.36)} & 81.94 \color{red}{(+1.49)} & 80.56 \color{red}{(+0.11)} \\
Toothbrush & 59.23 & 65.07 \color{red}{(+5.84)} & 60.71 \color{red}{(+1.48)} & 63.08 \color{red}{(+3.85)} & 59.56 \color{red}{(+0.33)} & 60.92 \color{red}{(+1.69)} & 62.80 \color{red}{(+3.57)} \\
Transistor & 36.75 & 61.32 \color{red}{(+24.57)} & 57.29 \color{red}{(+20.54)} & 51.68 \color{red}{(+14.93)} & 53.18 \color{red}{(+16.43)} & 62.78 \color{red}{(+26.03)} & 62.52 \color{red}{(+25.77)} \\
Wood & 50.92 & 54.71 \color{red}{(+3.79)} & 55.35 \color{red}{(+4.43)} & 54.66 \color{red}{(+3.74)} & 54.61 \color{red}{(+3.69)} & 54.74 \color{red}{(+3.82)} & 51.56 \color{red}{(+0.64)} \\
Zipper & 66.69 & 70.75 \color{red}{(+4.06)} & 69.77 \color{red}{(+3.08)} & 70.34 \color{red}{(+3.65)} & 70.35 \color{red}{(+3.66)} & 70.56 \color{red}{(+3.87)} & 68.91 \color{red}{(+2.22)} \\
\midrule
\multicolumn{8}{c}{\textbf{Model: DeepLabV3+}} \\
\midrule
Bottle & 79.72 & 82.04 \color{red}{(+2.32)} & 83.17 \color{red}{(+3.45)} & 82.58 \color{red}{(+2.86)} & 83.21 \color{red}{(+3.49)} & 81.44 \color{red}{(+1.72)} & 82.64 \color{red}{(+2.92)} \\
Cable & 57.52 & 59.22 \color{red}{(+1.70)} & 64.22 \color{red}{(+6.70)} & 62.42 \color{red}{(+4.90)} & 61.85 \color{red}{(+4.33)} & 62.79 \color{red}{(+5.27)} & 63.74 \color{red}{(+6.22)} \\
Capsule & 35.22 & 44.03 \color{red}{(+8.81)} & 46.50 \color{red}{(+11.28)} & 42.84 \color{red}{(+7.62)} & 42.14 \color{red}{(+6.92)} & 40.37 \color{red}{(+5.15)} & 44.69 \color{red}{(+9.47)} \\
Carpet & 64.30 & 66.00 \color{red}{(+1.70)} & 66.11 \color{red}{(+1.81)} & 66.58 \color{red}{(+2.28)} & 66.88 \color{red}{(+2.58)} & 66.78 \color{red}{(+2.48)} & 65.33 \color{red}{(+1.03)} \\
Grid & 50.73 & 56.17 \color{red}{(+5.44)} & 57.86 \color{red}{(+7.13)} & 55.51 \color{red}{(+4.78)} & 56.43 \color{red}{(+5.70)} & 56.93 \color{red}{(+6.20)} & 56.48 \color{red}{(+5.75)} \\
Hazelnut & 75.49 & 79.58 \color{red}{(+4.09)} & 77.15 \color{red}{(+1.66)} & 78.81 \color{red}{(+3.32)} & 75.91 \color{red}{(+0.42)} & 79.30 \color{red}{(+3.81)} & 81.06 \color{red}{(+5.57)} \\
Leather & 65.98 & 68.98 \color{red}{(+3.00)} & 68.94 \color{red}{(+2.96)} & 69.55 \color{red}{(+3.57)} & 68.73 \color{red}{(+2.75)} & 67.08 \color{red}{(+1.10)} & 67.56 \color{red}{(+1.58)} \\
Metal Nut & 91.50 & 91.92 \color{red}{(+0.42)} & 92.24 \color{red}{(+0.74)} & 91.64 \color{red}{(+0.14)} & 92.47 \color{red}{(+0.97)} & 92.16 \color{red}{(+0.66)} & 92.53 \color{red}{(+1.03)} \\
Pill & 76.66 & 79.01 \color{red}{(+2.35)} & 81.26 \color{red}{(+4.60)} & 80.06 \color{red}{(+3.40)} & 76.92 \color{red}{(+0.26)} & 77.33 \color{red}{(+0.67)} & 80.83 \color{red}{(+4.17)} \\
Screw & 47.20 & 50.36 \color{red}{(+3.16)} & 51.70 \color{red}{(+4.50)} & 51.73 \color{red}{(+4.53)} & 53.37 \color{red}{(+6.17)} & 54.90 \color{red}{(+7.70)} & 54.39 \color{red}{(+7.19)} \\
Tile & 84.90 & 86.10 \color{red}{(+1.20)} & 85.70 \color{red}{(+0.80)} & 86.28 \color{red}{(+1.38)} & 85.71 \color{red}{(+0.81)} & 86.01 \color{red}{(+1.11)} & 85.11 \color{red}{(+0.21)} \\
Toothbrush & 66.35 & 67.44 \color{red}{(+1.09)} & 70.28 \color{red}{(+3.93)} & 67.30 \color{red}{(+0.95)} & 68.65 \color{red}{(+2.30)} & 69.04 \color{red}{(+2.69)} & 69.35 \color{red}{(+3.00)} \\
Transistor & 47.92 & 48.89 \color{red}{(+0.97)} & 49.00 \color{red}{(+1.08)} & 66.01 \color{red}{(+18.09)} & 57.68 \color{red}{(+9.76)} & 60.45 \color{red}{(+12.53)} & 70.11 \color{red}{(+22.19)} \\
Wood & 61.96 & 66.75 \color{red}{(+4.79)} & 63.02 \color{red}{(+1.06)} & 66.95 \color{red}{(+4.99)} & 64.57 \color{red}{(+2.61)} & 66.42 \color{red}{(+4.46)} & 67.03 \color{red}{(+5.07)} \\
Zipper & 74.01 & 75.43 \color{red}{(+1.42)} & 75.51 \color{red}{(+1.50)} & 74.52 \color{red}{(+0.51)} & 75.11 \color{red}{(+1.10)} & 74.15 \color{red}{(+0.14)} & 74.72 \color{red}{(+0.71)} \\
\bottomrule
\end{tabular}
}
 
\caption{Segmentation mIoU performance comparison on the MVTec AD dataset. Part 1/3: Fast-SCNN and DeepLabV3+.}
\label{tab:mvtec_ad_segmentation_result_1}
\end{table*}

\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{l|c|cccccc}
\multicolumn{8}{c}{\textbf{Mvtec AD Part 2 / 3}} \\
\toprule
\textbf{Category} & \textbf{Baseline} & \textbf{Blistering} & \textbf{Corrosion} & \textbf{Dent} & \textbf{Peeling} & \textbf{Rust} & \textbf{Scratch} \\
\midrule
\multicolumn{8}{c}{\textbf{Model: UPerNet}} \\
\midrule
Bottle & 79.02 & 81.96 \color{red}{(+2.94)} & 82.10 \color{red}{(+3.08)} & 80.62 \color{red}{(+1.60)} & 80.58 \color{red}{(+1.56)} & 81.49 \color{red}{(+2.47)} & 81.01 \color{red}{(+1.99)} \\
Cable & 57.75 & 65.72 \color{red}{(+7.97)} & 62.76 \color{red}{(+5.01)} & 64.22 \color{red}{(+6.47)} & 62.12 \color{red}{(+4.37)} & 66.09 \color{red}{(+8.34)} & 61.83 \color{red}{(+4.08)} \\
Capsule & 28.83 & 41.53 \color{red}{(+12.70)} & 41.25 \color{red}{(+12.42)} & 30.71 \color{red}{(+1.88)} & 30.38 \color{red}{(+1.55)} & 40.26 \color{red}{(+11.43)} & 39.65 \color{red}{(+10.82)} \\
Carpet & 62.11 & 66.22 \color{red}{(+4.11)} & 67.24 \color{red}{(+5.13)} & 64.25 \color{red}{(+2.14)} & 64.14 \color{red}{(+2.03)} & 65.08 \color{red}{(+2.97)} & 65.17 \color{red}{(+3.06)} \\
Grid & 43.41 & 51.47 \color{red}{(+8.06)} & 54.64 \color{red}{(+11.23)} & 53.41 \color{red}{(+10.00)} & 52.07 \color{red}{(+8.66)} & 52.31 \color{red}{(+8.90)} & 55.66 \color{red}{(+12.25)} \\
Hazelnut & 65.98 & 75.63 \color{red}{(+9.65)} & 77.69 \color{red}{(+11.71)} & 74.97 \color{red}{(+8.99)} & 76.31 \color{red}{(+10.33)} & 75.30 \color{red}{(+9.32)} & 76.08 \color{red}{(+10.10)} \\
Leather & 60.19 & 64.24 \color{red}{(+4.05)} & 64.23 \color{red}{(+4.04)} & 61.78 \color{red}{(+1.59)} & 62.71 \color{red}{(+2.52)} & 62.30 \color{red}{(+2.11)} & 65.38 \color{red}{(+5.19)} \\
Metal Nut & 91.01 & 92.26 \color{red}{(+1.25)} & 91.33 \color{red}{(+0.32)} & 91.14 \color{red}{(+0.13)} & 91.83 \color{red}{(+0.82)} & 91.59 \color{red}{(+0.58)} & 91.57 \color{red}{(+0.56)} \\
Pill & 73.98 & 75.21 \color{red}{(+1.23)} & 77.46 \color{red}{(+3.48)} & 75.32 \color{red}{(+1.34)} & 76.40 \color{red}{(+2.42)} & 75.66 \color{red}{(+1.68)} & 77.62 \color{red}{(+3.64)} \\
Screw & 43.09 & 49.93 \color{red}{(+6.84)} & 50.45 \color{red}{(+7.36)} & 52.20 \color{red}{(+9.11)} & 45.83 \color{red}{(+2.74)} & 52.37 \color{red}{(+9.28)} & 53.29 \color{red}{(+10.20)} \\
Tile & 80.42 & 84.27 \color{red}{(+3.85)} & 85.17 \color{red}{(+4.75)} & 85.14 \color{red}{(+4.72)} & 85.03 \color{red}{(+4.61)} & 84.94 \color{red}{(+4.52)} & 85.29 \color{red}{(+4.87)} \\
Toothbrush & 65.28 & 68.56 \color{red}{(+3.28)} & 70.36 \color{red}{(+5.08)} & 67.28 \color{red}{(+2.00)} & 65.33 \color{red}{(+0.05)} & 67.54 \color{red}{(+2.26)} & 70.83 \color{red}{(+5.55)} \\
Transistor & 42.15 & 49.31 \color{red}{(+7.16)} & 56.51 \color{red}{(+14.36)} & 49.13 \color{red}{(+6.98)} & 68.42 \color{red}{(+26.27)} & 54.69 \color{red}{(+12.54)} & 66.44 \color{red}{(+24.29)} \\
Wood & 59.68 & 60.10 \color{red}{(+0.42)} & 59.71 \color{red}{(+0.03)} & 62.00 \color{red}{(+2.32)} & 63.42 \color{red}{(+3.74)} & 63.86 \color{red}{(+4.18)} & 60.65 \color{red}{(+0.97)} \\
Zipper & 72.97 & 73.25 \color{red}{(+0.28)} & 73.68 \color{red}{(+0.71)} & 73.75 \color{red}{(+0.78)} & 73.61 \color{red}{(+0.64)} & 74.76 \color{red}{(+1.79)} & 73.07 \color{red}{(+0.10)} \\
\midrule
\multicolumn{8}{c}{\textbf{Model: SegFormer}} \\
\midrule
Bottle & 24.83 & 72.60 \color{red}{(+47.77)} & 79.43 \color{red}{(+54.60)} & 77.56 \color{red}{(+52.73)} & 76.70 \color{red}{(+51.87)} & 25.53 \color{red}{(+0.70)} & 78.19 \color{red}{(+53.36)} \\
Cable & 33.15 & 56.54 \color{red}{(+23.39)} & 47.52 \color{red}{(+14.37)} & 55.57 \color{red}{(+22.42)} & 56.82 \color{red}{(+23.67)} & 53.66 \color{red}{(+20.51)} & 55.30 \color{red}{(+22.15)} \\
Capsule & 2.35 & 9.03 \color{red}{(+6.68)} & 22.42 \color{red}{(+20.07)} & 15.75 \color{red}{(+13.40)} & 29.58 \color{red}{(+27.23)} & 15.92 \color{red}{(+13.57)} & 28.84 \color{red}{(+26.49)} \\
Carpet & 60.19 & 62.60 \color{red}{(+2.41)} & 62.35 \color{red}{(+2.16)} & 64.41 \color{red}{(+4.22)} & 63.61 \color{red}{(+3.42)} & 64.82 \color{red}{(+4.63)} & 60.36 \color{red}{(+0.17)} \\
Grid & 33.87 & 43.04 \color{red}{(+9.17)} & 46.13 \color{red}{(+12.26)} & 34.06 \color{red}{(+0.19)} & 48.39 \color{red}{(+14.52)} & 46.53 \color{red}{(+12.66)} & 36.95 \color{red}{(+3.08)} \\
Hazelnut & 53.93 & 72.85 \color{red}{(+18.92)} & 71.26 \color{red}{(+17.33)} & 70.80 \color{red}{(+16.87)} & 69.44 \color{red}{(+15.51)} & 73.27 \color{red}{(+19.34)} & 68.41 \color{red}{(+14.48)} \\
Leather & 52.87 & 65.82 \color{red}{(+12.95)} & 64.67 \color{red}{(+11.80)} & 64.75 \color{red}{(+11.88)} & 53.34 \color{red}{(+0.47)} & 61.31 \color{red}{(+8.44)} & 63.53 \color{red}{(+10.66)} \\
Metal Nut & 88.32 & 89.71 \color{red}{(+1.39)} & 90.69 \color{red}{(+2.37)} & 88.55 \color{red}{(+0.23)} & 90.38 \color{red}{(+2.06)} & 89.70 \color{red}{(+1.38)} & 91.77 \color{red}{(+3.45)} \\
Pill & 54.30 & 62.61 \color{red}{(+8.31)} & 61.01 \color{red}{(+6.71)} & 55.37 \color{red}{(+1.07)} & 61.83 \color{red}{(+7.53)} & 75.68 \color{red}{(+21.38)} & 70.54 \color{red}{(+16.24)} \\
Screw & 21.64 & 47.38 \color{red}{(+25.74)} & 46.87 \color{red}{(+25.23)} & 46.26 \color{red}{(+24.62)} & 34.23 \color{red}{(+12.59)} & 50.21 \color{red}{(+28.57)} & 46.50 \color{red}{(+24.86)} \\
Tile & 82.16 & 83.09 \color{red}{(+0.93)} & 83.31 \color{red}{(+1.15)} & 83.45 \color{red}{(+1.29)} & 83.44 \color{red}{(+1.28)} & 83.04 \color{red}{(+0.88)} & 82.20 \color{red}{(+0.04)} \\
Toothbrush & 58.30 & 64.80 \color{red}{(+6.50)} & 63.09 \color{red}{(+4.79)} & 58.33 \color{red}{(+0.03)} & 65.21 \color{red}{(+6.91)} & 58.85 \color{red}{(+0.55)} & 66.49 \color{red}{(+8.19)} \\
Transistor & 20.31 & 67.38 \color{red}{(+47.07)} & 39.72 \color{red}{(+19.41)} & 39.28 \color{red}{(+18.97)} & 37.29 \color{red}{(+16.98)} & 41.18 \color{red}{(+20.87)} & 26.00 \color{red}{(+5.69)} \\
Wood & 51.19 & 60.04 \color{red}{(+8.85)} & 56.07 \color{red}{(+4.88)} & 58.36 \color{red}{(+7.17)} & 62.44 \color{red}{(+11.25)} & 60.28 \color{red}{(+9.09)} & 62.06 \color{red}{(+10.87)} \\
Zipper & 69.36 & 71.34 \color{red}{(+1.98)} & 71.69 \color{red}{(+2.33)} & 71.49 \color{red}{(+2.13)} & 69.99 \color{red}{(+0.63)} & 73.95 \color{red}{(+4.59)} & 72.09 \color{red}{(+2.73)} \\
\bottomrule
\end{tabular}
}
\caption{Segmentation mIoU performance comparison on the MVTec AD dataset. Part 2/3: UPerNet and SegFormer.}
\label{tab:mvtec_ad_segmentation_result_2}
\end{table*}

\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{l|c|cccccc}
\multicolumn{8}{c}{\textbf{Mvtec AD Part 3 / 3}} \\
\toprule
\textbf{Category} & \textbf{Baseline} & \textbf{Blistering} & \textbf{Corrosion} & \textbf{Dent} & \textbf{Peeling} & \textbf{Rust} & \textbf{Scratch} \\
\midrule
\multicolumn{8}{c}{\textbf{Model: PSPNet}} \\
\midrule
Bottle & 80.83 & 80.89 \color{red}{(+0.06)} & 82.65 \color{red}{(+1.82)} & 81.46 \color{red}{(+0.63)} & 81.73 \color{red}{(+0.90)} & 81.64 \color{red}{(+0.81)} & 81.30 \color{red}{(+0.47)} \\
Cable & 50.67 & 60.90 \color{red}{(+10.23)} & 57.13 \color{red}{(+6.46)} & 60.39 \color{red}{(+9.72)} & 61.67 \color{red}{(+11.00)} & 57.80 \color{red}{(+7.13)} & 65.16 \color{red}{(+14.49)} \\
Capsule & 20.81 & 25.36 \color{red}{(+4.55)} & 37.79 \color{red}{(+16.98)} & 40.33 \color{red}{(+19.52)} & 43.15 \color{red}{(+22.34)} & 42.04 \color{red}{(+21.23)} & 40.25 \color{red}{(+19.44)} \\
Carpet & 62.62 & 65.57 \color{red}{(+2.95)} & 66.35 \color{red}{(+3.73)} & 63.24 \color{red}{(+0.62)} & 67.72 \color{red}{(+5.10)} & 65.15 \color{red}{(+2.53)} & 63.12 \color{red}{(+0.50)} \\
Grid & 46.55 & 51.89 \color{red}{(+5.34)} & 49.20 \color{red}{(+2.65)} & 52.85 \color{red}{(+6.30)} & 50.18 \color{red}{(+3.63)} & 46.77 \color{red}{(+0.22)} & 48.56 \color{red}{(+2.01)} \\
Hazelnut & 70.13 & 74.06 \color{red}{(+3.93)} & 76.37 \color{red}{(+6.24)} & 72.96 \color{red}{(+2.83)} & 74.92 \color{red}{(+4.79)} & 75.30 \color{red}{(+5.17)} & 77.64 \color{red}{(+7.51)} \\
Leather & 57.34 & 62.07 \color{red}{(+4.73)} & 58.61 \color{red}{(+1.27)} & 65.02 \color{red}{(+7.68)} & 60.51 \color{red}{(+3.17)} & 60.72 \color{red}{(+3.38)} & 61.60 \color{red}{(+4.26)} \\
Metal Nut & 90.56 & 92.17 \color{red}{(+1.61)} & 91.43 \color{red}{(+0.87)} & 91.04 \color{red}{(+0.48)} & 91.26 \color{red}{(+0.70)} & 91.23 \color{red}{(+0.67)} & 91.54 \color{red}{(+0.98)} \\
Pill & 70.94 & 78.48 \color{red}{(+7.54)} & 81.40 \color{red}{(+10.46)} & 77.76 \color{red}{(+6.82)} & 76.73 \color{red}{(+5.79)} & 76.87 \color{red}{(+5.93)} & 75.90 \color{red}{(+4.96)} \\
Screw & 44.61 & 48.21 \color{red}{(+3.60)} & 48.62 \color{red}{(+4.01)} & 53.82 \color{red}{(+9.21)} & 50.09 \color{red}{(+5.48)} & 50.82 \color{red}{(+6.21)} & 50.23 \color{red}{(+5.62)} \\
Tile & 82.77 & 84.29 \color{red}{(+1.52)} & 84.60 \color{red}{(+1.83)} & 85.46 \color{red}{(+2.69)} & 84.95 \color{red}{(+2.18)} & 85.24 \color{red}{(+2.47)} & 84.83 \color{red}{(+2.06)} \\
Toothbrush & 65.52 & 66.12 \color{red}{(+0.60)} & 65.88 \color{red}{(+0.36)} & 65.55 \color{red}{(+0.03)} & 66.99 \color{red}{(+1.47)} & 66.09 \color{red}{(+0.57)} & 66.03 \color{red}{(+0.51)} \\
Transistor & 46.13 & 46.30 \color{red}{(+0.17)} & 60.78 \color{red}{(+14.65)} & 66.07 \color{red}{(+19.94)} & 58.23 \color{red}{(+12.10)} & 61.97 \color{red}{(+15.84)} & 65.73 \color{red}{(+19.60)} \\
Wood & 59.70 & 64.03 \color{red}{(+4.33)} & 60.89 \color{red}{(+1.19)} & 64.77 \color{red}{(+5.07)} & 66.42 \color{red}{(+6.72)} & 60.73 \color{red}{(+1.03)} & 62.58 \color{red}{(+2.88)} \\
Zipper & 72.52 & 72.73 \color{red}{(+0.21)} & 73.80 \color{red}{(+1.28)} & 74.98 \color{red}{(+2.46)} & 74.23 \color{red}{(+1.71)} & 72.72 \color{red}{(+0.20)} & 74.02 \color{red}{(+1.50)} \\
\midrule
\multicolumn{8}{c}{\textbf{Model: MobileNet V3}} \\
\midrule
Bottle & 77.62 & 80.67 \color{red}{(+3.05)} & 78.45 \color{red}{(+0.83)} & 80.45 \color{red}{(+2.83)} & 80.26 \color{red}{(+2.64)} & 80.89 \color{red}{(+3.27)} & 82.26 \color{red}{(+4.64)} \\
Cable & 67.97 & 71.70 \color{red}{(+3.73)} & 70.78 \color{red}{(+2.81)} & 71.67 \color{red}{(+3.70)} & 71.61 \color{red}{(+3.64)} & 71.32 \color{red}{(+3.35)} & 69.57 \color{red}{(+1.60)} \\
Capsule & 48.02 & 48.19 \color{red}{(+0.17)} & 51.09 \color{red}{(+3.07)} & 50.45 \color{red}{(+2.43)} & 50.33 \color{red}{(+2.31)} & 52.65 \color{red}{(+4.63)} & 52.47 \color{red}{(+4.45)} \\
Carpet & 66.72 & 66.86 \color{red}{(+0.14)} & 68.83 \color{red}{(+2.11)} & 67.65 \color{red}{(+0.93)} & 67.88 \color{red}{(+1.16)} & 68.02 \color{red}{(+1.30)} & 67.28 \color{red}{(+0.56)} \\
Grid & 49.63 & 52.90 \color{red}{(+3.27)} & 51.02 \color{red}{(+1.39)} & 50.29 \color{red}{(+0.66)} & 53.32 \color{red}{(+3.69)} & 53.03 \color{red}{(+3.40)} & 50.75 \color{red}{(+1.12)} \\
Hazelnut & 73.43 & 78.98 \color{red}{(+5.55)} & 79.08 \color{red}{(+5.65)} & 73.74 \color{red}{(+0.31)} & 76.47 \color{red}{(+3.04)} & 77.37 \color{red}{(+3.94)} & 77.94 \color{red}{(+4.51)} \\
Leather & 68.95 & 71.48 \color{red}{(+2.53)} & 70.99 \color{red}{(+2.04)} & 70.57 \color{red}{(+1.62)} & 70.71 \color{red}{(+1.76)} & 70.99 \color{red}{(+2.04)} & 71.90 \color{red}{(+2.95)} \\
Metal Nut & 89.35 & 92.02 \color{red}{(+2.67)} & 91.16 \color{red}{(+1.81)} & 91.10 \color{red}{(+1.75)} & 91.70 \color{red}{(+2.35)} & 91.88 \color{red}{(+2.53)} & 91.04 \color{red}{(+1.69)} \\
Pill & 77.86 & 78.76 \color{red}{(+0.90)} & 80.93 \color{red}{(+3.07)} & 78.79 \color{red}{(+0.93)} & 79.99 \color{red}{(+2.13)} & 79.83 \color{red}{(+1.97)} & 80.26 \color{red}{(+2.40)} \\
Screw & 49.52 & 53.69 \color{red}{(+4.17)} & 53.46 \color{red}{(+3.94)} & 53.04 \color{red}{(+3.52)} & 49.61 \color{red}{(+0.09)} & 53.38 \color{red}{(+3.86)} & 52.41 \color{red}{(+2.89)} \\
Tile & 83.85 & 84.68 \color{red}{(+0.83)} & 84.21 \color{red}{(+0.36)} & 84.40 \color{red}{(+0.55)} & 85.49 \color{red}{(+1.64)} & 84.20 \color{red}{(+0.35)} & 85.92 \color{red}{(+2.07)} \\
Toothbrush & 59.66 & 66.47 \color{red}{(+6.81)} & 63.17 \color{red}{(+3.51)} & 66.89 \color{red}{(+7.23)} & 64.76 \color{red}{(+5.10)} & 62.13 \color{red}{(+2.47)} & 59.69 \color{red}{(+0.03)} \\
Transistor & 60.18 & 63.99 \color{red}{(+3.81)} & 73.09 \color{red}{(+12.91)} & 70.59 \color{red}{(+10.41)} & 71.94 \color{red}{(+11.76)} & 68.00 \color{red}{(+7.82)} & 72.89 \color{red}{(+12.71)} \\
Wood & 64.12 & 67.04 \color{red}{(+2.92)} & 67.56 \color{red}{(+3.44)} & 67.62 \color{red}{(+3.50)} & 66.59 \color{red}{(+2.47)} & 66.82 \color{red}{(+2.70)} & 65.80 \color{red}{(+1.68)} \\
Zipper & 74.98 & 75.08 \color{red}{(+0.10)} & 75.79 \color{red}{(+0.81)} & 75.53 \color{red}{(+0.55)} & 75.79 \color{red}{(+0.81)} & 75.81 \color{red}{(+0.83)} & 75.67 \color{red}{(+0.69)} \\
\bottomrule
\end{tabular}
}
\caption{Segmentation mIoU performance comparison on the MVTec AD dataset. Part 3/3: PSPNet and MobileNet V3.}
\label{tab:mvtec_ad_segmentation_result_3}
\end{table*}










\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{0.8}
% 열 정의: 왼쪽 4개 (모델, Base, Ours, 상승폭) | 오른쪽 4개 (모델, Base, Ours, 상승폭)
\begin{tabular}{lccc|lccc}
\multicolumn{8}{c}{\textbf{KolektorSDD}} \\
\toprule
\textbf{Model} & \textbf{Baseline} & \textbf{Ours} & \textbf{$\Delta$} & \textbf{Model} & \textbf{Baseline} & \textbf{Ours} & \textbf{$\Delta$} \\
\midrule
% 첫 번째 줄: Rust & Scratch
\multicolumn{4}{c|}{\textbf{Rust}} & \multicolumn{4}{c}{\textbf{Scratch}} \\
\midrule
PSPNet & 81.05\% & 84.89\% & \textcolor{red}{\textbf{(+3.84)}} & BEiT & 71.41\% & 71.99\% & \textcolor{red}{\textbf{(+0.59)}} \\
BiSeNetV2 & 75.85\% & 81.24\% & \textcolor{red}{\textbf{(+5.39)}} & BiSeNetV2 & 80.81\% & 84.29\% & \textcolor{red}{\textbf{(+3.48)}} \\
APCNet & 80.21\% & 85.74\% & \textcolor{red}{\textbf{(+5.53)}} & UPerNet & 80.77\% & 84.68\% & \textcolor{red}{\textbf{(+3.91)}} \\
CCNet & 79.99\% & 86.04\% & \textcolor{red}{\textbf{(+6.05)}} & MobileNetV3 & 69.54\% & 73.86\% & \textcolor{red}{\textbf{(+4.32)}} \\
UPerNet & 80.77\% & 87.16\% & \textcolor{red}{\textbf{(+6.39)}} & PSPNet & 81.05\% & 85.77\% & \textcolor{red}{\textbf{(+4.72)}} \\
DANet & 79.11\% & 85.57\% & \textcolor{red}{\textbf{(+6.46)}} & ViT & 80.55\% & 85.49\% & \textcolor{red}{\textbf{(+4.94)}} \\
GCNet & 77.79\% & 85.42\% & \textcolor{red}{\textbf{(+7.63)}} & CCNet & 79.99\% & 86.20\% & \textcolor{red}{\textbf{(+6.21)}} \\
ANN & 77.73\% & 86.24\% & \textcolor{red}{\textbf{(+8.51)}} & APCNet & 80.21\% & 86.53\% & \textcolor{red}{\textbf{(+6.32)}} \\
ViT & 68.97\% & 79.31\% & \textcolor{red}{\textbf{(+10.34)}} & GCNet & 77.79\% & 85.23\% & \textcolor{red}{\textbf{(+7.44)}} \\
BEiT & 68.01\% & 79.22\% & \textcolor{red}{\textbf{(+11.21)}} & DANet & 79.11\% & 86.86\% & \textcolor{red}{\textbf{(+7.75)}} \\
MobileNetV3 & 69.54\% & 81.32\% & \textcolor{red}{\textbf{(+11.78)}} & ANN & 77.73\% & 86.80\% & \textcolor{red}{\textbf{(+9.07)}} \\
Fast-SCNN & 75.00\% & 87.44\% & \textcolor{red}{\textbf{(+12.44)}} & Fast-SCNN & 63.86\% & 90.76\% & \textcolor{red}{\textbf{(+26.90)}} \\
\midrule
% 두 번째 줄: Blistering & Corrosion
\multicolumn{4}{c|}{\textbf{Blistering}} & \multicolumn{4}{c}{\textbf{Corrosion}} \\
\midrule
BEiT & 71.12\% & 72.60\% & \textcolor{red}{\textbf{(+1.48)}} & BEiT & 71.72\% & 72.31\% & \textcolor{red}{\textbf{(+0.59)}} \\
BiSeNetV2 & 81.90\% & 85.32\% & \textcolor{red}{\textbf{(+3.42)}} & BiSeNetV2 & 84.63\% & 87.34\% & \textcolor{red}{\textbf{(+2.71)}} \\
UPerNet & 80.77\% & 84.89\% & \textcolor{red}{\textbf{(+4.12)}} & UPerNet & 80.77\% & 84.67\% & \textcolor{red}{\textbf{(+3.90)}} \\
ViT & 80.55\% & 85.48\% & \textcolor{red}{\textbf{(+4.93)}} & ViT & 80.55\% & 85.52\% & \textcolor{red}{\textbf{(+4.97)}} \\
APCNet & 80.21\% & 85.90\% & \textcolor{red}{\textbf{(+5.69)}} & CCNet & 79.99\% & 85.32\% & \textcolor{red}{\textbf{(+5.33)}} \\
PSPNet & 81.05\% & 86.89\% & \textcolor{red}{\textbf{(+5.84)}} & DANet & 79.11\% & 84.55\% & \textcolor{red}{\textbf{(+5.44)}} \\
CCNet & 79.99\% & 86.45\% & \textcolor{red}{\textbf{(+6.46)}} & APCNet & 80.21\% & 85.84\% & \textcolor{red}{\textbf{(+5.63)}} \\
DANet & 79.11\% & 86.53\% & \textcolor{red}{\textbf{(+7.42)}} & PSPNet & 81.05\% & 87.44\% & \textcolor{red}{\textbf{(+6.39)}} \\
GCNet & 77.79\% & 85.74\% & \textcolor{red}{\textbf{(+7.95)}} & MobileNetV3 & 69.54\% & 76.95\% & \textcolor{red}{\textbf{(+7.41)}} \\
ANN & 77.73\% & 86.48\% & \textcolor{red}{\textbf{(+8.75)}} & GCNet & 77.79\% & 85.91\% & \textcolor{red}{\textbf{(+8.12)}} \\
MobileNetV3 & 69.54\% & 79.48\% & \textcolor{red}{\textbf{(+9.94)}} & ANN & 77.73\% & 86.98\% & \textcolor{red}{\textbf{(+9.25)}} \\
Fast-SCNN & 64.71\% & 90.62\% & \textcolor{red}{\textbf{(+25.91)}} & Fast-SCNN & 61.38\% & 91.77\% & \textcolor{red}{\textbf{(+30.39)}} \\
\midrule
% 세 번째 줄: Dent & Peeling
\multicolumn{4}{c|}{\textbf{Dent}} & \multicolumn{4}{c}{\textbf{Peeling}} \\
\midrule
BEiT & 71.97\% & 72.58\% & \textcolor{red}{\textbf{(+0.61)}} & BEiT & 71.46\% & 72.08\% & \textcolor{red}{\textbf{(+0.62)}} \\
CCNet & 79.99\% & 84.51\% & \textcolor{red}{\textbf{(+4.52)}} & PSPNet & 81.05\% & 84.92\% & \textcolor{red}{\textbf{(+3.87)}} \\
ViT & 80.55\% & 85.57\% & \textcolor{red}{\textbf{(+5.02)}} & BiSeNetV2 & 82.94\% & 87.25\% & \textcolor{red}{\textbf{(+4.31)}} \\
UPerNet & 80.77\% & 86.04\% & \textcolor{red}{\textbf{(+5.27)}} & ViT & 80.55\% & 85.13\% & \textcolor{red}{\textbf{(+4.58)}} \\
APCNet & 80.21\% & 86.61\% & \textcolor{red}{\textbf{(+6.40)}} & UPerNet & 80.77\% & 85.36\% & \textcolor{red}{\textbf{(+4.59)}} \\
PSPNet & 81.05\% & 87.65\% & \textcolor{red}{\textbf{(+6.60)}} & CCNet & 79.99\% & 85.29\% & \textcolor{red}{\textbf{(+5.30)}} \\
DANet & 79.11\% & 85.73\% & \textcolor{red}{\textbf{(+6.62)}} & DANet & 79.11\% & 85.31\% & \textcolor{red}{\textbf{(+6.20)}} \\
GCNet & 77.79\% & 85.81\% & \textcolor{red}{\textbf{(+8.02)}} & GCNet & 77.79\% & 85.25\% & \textcolor{red}{\textbf{(+7.46)}} \\
BiSeNetV2 & 81.76\% & 90.66\% & \textcolor{red}{\textbf{(+8.90)}} & MobileNetV3 & 69.54\% & 77.04\% & \textcolor{red}{\textbf{(+7.50)}} \\
MobileNetV3 & 69.54\% & 79.20\% & \textcolor{red}{\textbf{(+9.66)}} & APCNet & 80.21\% & 88.19\% & \textcolor{red}{\textbf{(+7.98)}} \\
ANN & 77.73\% & 87.70\% & \textcolor{red}{\textbf{(+9.97)}} & ANN & 77.73\% & 86.87\% & \textcolor{red}{\textbf{(+9.14)}} \\
Fast-SCNN & 66.84\% & 90.00\% & \textcolor{red}{\textbf{(+23.16)}} & Fast-SCNN & 66.30\% & 89.95\% & \textcolor{red}{\textbf{(+23.65)}} \\
\bottomrule
\end{tabular}
}

\caption{Segmentation mIoU performance comparison on the KolektorSDD dataset.}
\label{tab:kolektor_sdd_ad_segmentation_result}
\end{table*}


\subsection{Defect Segmentation: Implementation and Performance}

We conduct a series of preliminary experiments using Fast-SCNN and MobileNetV3 due to their fast training speeds, serving as the basis for optimizing our training strategy. Re-labeling experiments in Table~\ref{tab:comparison_re-labeling} demonstrate improved performance across nearly all metrics for both models. \textcolor{luckyred}{To address the severe label imbalance in the segmentation dataset (Table~\ref{tab:Carbide Insert, Training and testing dataset for segmentation.}), we evaluated various loss functions. As shown in Table~\ref{tab:comparison_loss}, switching to Focal Loss with $\alpha = [0.1, 0.3, 0.6]$ and $\gamma=2$ yielded robust IoU scores for dense segmentation tasks. Consequently, we select Focal Loss as the primary loss function, as it consistently outperformed cross-entropy in handling densely segmented defect regions.}

\textcolor{luckyred}{Table~\ref{tab:carbie_segmentation_result} presents the segmentation results on the Carbide Insert dataset. Our method, particularly when using the specific `Peeling' prompt, significantly improved performance over the baseline. Traditional augmentation techniques alone were insufficient for capturing fine-grained defect boundaries. In contrast, our approach achieved an average IoU increase of 22.8\% in the \textit{Attached} defect region and 8.6\% in the \textit{Broken} region across most models. Notably, lightweight models like Fast-SCNN showed a substantial accuracy gain (+22.82\%), while hierarchical models like UPerNet demonstrated the highest improvement in Attached IoU (+19.66\%), proving the efficacy of semantically targeted synthesis.}

\textcolor{luckyred}{We further validat the generalization capability of our framework on standard benchmarks, MVTec AD and KolektorSDD. As detailed in Tables~\ref{tab:mvtec_ad_segmentation_result_1}, \ref{tab:mvtec_ad_segmentation_result_2}, and \ref{tab:mvtec_ad_segmentation_result_3}, category-specific prompt engineering (e.g., `Corrosion' for metal) yielded significant gains, with SegFormer achieving up to +54.60\% improvement in specific categories. Similarly, for the KolektorSDD dataset (Table~\ref{tab:kolektor_sdd_ad_segmentation_result}), prompts like `Corrosion' and `Scratch' enabled Fast-SCNN to achieve a +30.39\% performance boost. These results confirm that our method effectively compensates for data scarcity and enhances structural diversity, benefiting both lightweight and complex architectures.}





\begin{table}[]
\centering
\renewcommand{\arraystretch}{0.4}
\setlength{\extrarowheight}{8pt}
\begin{tabular}{l|c|ccc}
\toprule
\textbf{Location} & \textbf{RLI} & \textbf{PSNR$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{LPIPS$\downarrow$} \\
\midrule
- & \ding{55} & 26.82 & 0.804 & 0.249 \\
DOWN & \ding{51} & \underline{27.67} & \underline{0.820} & 0.\underline{211} \\
MID & \ding{51} & 27.34 & 0.793 & 0.263 \\
UP & \ding{51} & \textbf{30.47} & \textbf{0.855} & \textbf{0.150} \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison of reconstruction quality based on the location of RLI connections.}
\label{tab:rli_exp}
\end{table}







\begin{table}[]
\centering
\renewcommand{\arraystretch}{0.5}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{PSNR~$\uparrow$ }} & \multicolumn{2}{c|}{\textbf{SSIM~$\uparrow$}} & \multicolumn{2}{c}{\textbf{LPIPS~$\downarrow$}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & \textbf{Base / RLI} & \textbf{Gain ($\Delta$)} & \textbf{Base / RLI} & \textbf{Gain ($\Delta$)} & \textbf{Base / RLI} & \textbf{Gain ($\Delta$)} \\
\midrule
SD 1.4 & 29.61 / \textbf{29.73} & \textcolor{red}{\small{0.41\% $\uparrow$}} & 0.8251 / \textbf{0.8273} & \textcolor{red}{\small{0.27\% $\uparrow$}} & 0.0812 / \textbf{0.0793} & \textcolor{red}{\small{2.34\% $\downarrow$}} \\
SD 2.0 & 30.72 / \textbf{30.95} & \textcolor{red}{\small{0.75\% $\uparrow$}} & 0.8443 / \textbf{0.8465} & \textcolor{red}{\small{0.26\% $\uparrow$}} & 0.0501 / \textbf{0.0482} & \textcolor{red}{\small{3.79\% $\downarrow$}} \\
SD 2.1 & 29.70 / \textbf{30.44} & \textcolor{red}{\small{2.49\% $\uparrow$}} & 0.8364 / \textbf{0.8436} & \textcolor{red}{\small{0.86\% $\uparrow$}} & 0.0613 / \textbf{0.0524} & \textcolor{red}{\small{14.52\% $\downarrow$}} \\
SDXL & 24.03 / \textbf{30.77} & \textcolor{red}{\small{28.05\% $\uparrow$}} & 0.7472 / \textbf{0.8513} & \textcolor{red}{\small{13.93\% $\uparrow$}} & 0.2323 / \textbf{0.0581} & \textcolor{red}{\small{75.00\% $\downarrow$}} \\
\bottomrule
\end{tabular}
}
\caption{Comparison of reconstruction performance with and
without RLI across Stable Diffusion models.}
\label{tab:rli_combined_single}
\end{table}


\section{Ablation Study}
\label{sec:ablation_study}

\subsection{Ablation Study on Residual Linear Interpolation (RLI)}
\label{sec:rli_ablation}
\textcolor{luckyred}{
To rigorously validate the efficacy of the proposed Residual Linear Interpolation (RLI) and determine its optimal configuration, we first conduct a granular ablation study investigating the impact of applying RLI to different blocks of the U-Net architecture. As detailed in Table~\ref{tab:rli_exp}, the location of RLI significantly influences reconstruction quality: applying RLI solely to the Down or Mid blocks yields negligible improvement or even degradation (e.g., SSIM decreases to 0.793 in the Mid-block), suggesting that structural constraints during the semantic compression phase interfere with latent abstraction. In contrast, applying RLI exclusively to the Up-blocks achieves the highest performance (PSNR \textbf{30.47 dB}, LPIPS \textbf{0.150}), confirming that the decoding phase is critical for recovering high-frequency details a finding visually corroborated by the artifact-free reconstructions in Figure~\ref{fig:rli_exp}. Building upon this optimal 'Up-block only' configuration, we further validate RLI across architectures ranging from SD 1.4 to SDXL. As shown in Table~\ref{tab:rli_combined_single}, while RLI consistently enhances fidelity across all versions, the improvement is most significant in SDXL, yielding a dramatic 28.05\% increase in PSNR (from 24.03 dB to 30.77 dB) and a critical 75.00\% reduction in LPIPS (from 0.2323 to 0.0581), thereby confirming its indispensable role in minimizing perceptual structural differences in large-scale models.
}


\begin{figure*}[p] % figure 환경으로 변경
    \centering
    % 이미지가 너무 크면 한 페이지에 안 들어갈 수 있으므로 width 조절 권장
    \includegraphics[width=1.0\linewidth]{fig/down_mid_up_circle.png} 
    \caption{Visual ablation study on RLI locations.} % 일반 caption 사용
    \label{fig:rli_exp}
\end{figure*}


    

\begin{figure*}[] % figure 환경으로 변경
    \centering
    % 이미지가 너무 크면 한 페이지에 안 들어갈 수 있으므로 width 조절 권장
    \includegraphics[width=0.8\linewidth]{fig/prompt_exp_circle3.png} 
    \caption{Qualitative comparison of reconstruction quality across prompt specificities.} % 일반 caption 사용
    \label{fig:prompt_exp}
\end{figure*}











\subsection{Isolating the Role of Text Prompts: Robustness and Specificity}
\label{sec:prompt_robustness}
\textcolor{luckyred}{
A primary concern in text-guided synthesis is the model's sensitivity to prompt selection. In our preliminary work~\cite{DISN}, we demonstrated that Null Embedding Optimization effectively decouples structural fidelity from semantic guidance. Experiments using completely irrelevant prompts, such as ``photo of a golden retriever,'' yielded a PSNR of 28.57 dB and SSIM of 0.873, which are statistically comparable to the optimal prompt's performance (PSNR 28.67 dB). This marginal difference ($<0.1$ dB) confirms that our optimization rigorously constrains the latent space, ensuring that the geometric integrity of the industrial part is preserved even when the text prompt is semantically noisy.
}
\textcolor{luckyred}{
As shown in Fig~\ref{fig:prompt_exp}, While the model exhibits strong robustness to suboptimal text, we further investigated whether increasing prompt specificity could enhance high-fidelity reconstruction in diverse domains. We extend the evaluation to standard benchmarks, MVTec AD and KolektorSDD, using three distinct levels of prompt specificity: (1) \textbf{Low} (e.g., ``scratch''), (2) \textbf{Moderate} (e.g., ``photo of peeling defect''), and (3) \textbf{High} (e.g., ``A defect on the gray surface with a clustered black crack in the central area''). This setup aims to verify if the model can leverage granular textual cues beyond maintaining baseline robustness.
}
\textcolor{luckyred}{
As presented in Table~\ref{tab:prompt_specificity}, increasing prompt specificity yields additional performance gains. Notably, for the KolektorSDD dataset, transitioning from a \textit{Low} to a \textit{High} prompt resulted in a PSNR increase from 30.03 dB to 34.17 dB. Despite this potential for maximization, we deliberately adopt the \textit{Moderate} length prompt as the standard configuration for our framework. This decision stems from the practical reality of industrial applications, where constructing highly detailed, case-specific descriptions (\textit{High}) varies significantly across different manufacturing sites. By selecting the \textit{Moderate} level, we demonstrate a robust performance baseline that balances reconstruction quality with operational feasibility, while leaving the option for further optimization through detailed prompt engineering to the discretion of on-site operators.
}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{0.5}
\setlength{\extrarowheight}{8pt}
\begin{tabular}{c|c|ccc}
\toprule
\textbf{Data}                                                               & \textbf{Prompt Specificity} & \textbf{PSNR$\uparrow$} & \textbf{SSIM$\uparrow$} & \textbf{LPIPS$\downarrow$} \\
\midrule
\multirow{3}{*}{Carbide Insert} & Low & 35.91 & 0.958  & 0.095   \\
                                                                    & Moderate & \underline{36.79} & \underline{0.963}  & \underline{0.075}   \\
                                                                    & High & \textbf{38.23} & \textbf{0.968}  & \textbf{0.068}   \\
                                                                    \midrule
\multirow{3}{*}{MvTec AD}                                              & Low & 29.08 & 0.846  & 0.169   \\
                                                                    & Moderate & \underline{30.45} & \underline{0.858}  & \underline{0.151}   \\
                                                                    & High & \textbf{30.90} & \textbf{0.862}  & \textbf{0.143}   \\
                                                                    \midrule
\multirow{3}{*}{KolektorSDD}                                           & Low & 30.03 & 0.958 & 0.124   \\
                                                                    & Moderate & \underline{32.05} & \underline{0.964} & \underline{0.120}   \\
                                                                    & High & \textbf{34.17} & \textbf{0.966} & \textbf{0.113}   \\
                                                                    \bottomrule
\end{tabular}
\caption{Quantitative comparison of reconstruction quality across prompt specificities.}
\label{tab:prompt_specificity}
\end{table}



% ===================================================











% ===================================================
% Conclusion
% ===================================================
\section{Conclusion}
\textcolor{luckyred}{In this work, we addressed industrial data scarcity by introducing a framework that integrates Null Embedding Optimization with Residual Linear Interpolation (RLI). This deterministic approach mitigates structural inconsistencies inherent in diffusion models, preserving fine-grained geometric integrity while enabling diverse defect morphology. Crucially, our method achieves high-fidelity synthesis without fine-tuning or LoRA, yielding substantial gains in downstream classification and segmentation tasks. Future research will focus on reducing inference latency and integrating Large Language Models (LLMs) to automate prompt generation for fully automated data augmentation.}






% ===================================================
% Acknowledgment
% ===================================================
\section*{Acknowledgment} 
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2022-00155915, Artificial Intelligence Convergence Innovation Human Resources Development (Inha University) and No.2021-0-02068, Artificial Intelligence Innovation Hub and IITP-2024-RS-2024-00360227, Leading Generative AI Human Resources Development).


% ===================================================
% bibliography
% ===================================================
 \bibliographystyle{elsarticle-num-names} 
 \bibliography{main}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

% \begin{thebibliography}{00}

% %% For authoryear reference style
% %% \bibitem[Author(year)]{label}
% %% Text of bibliographic item

% \bibitem[Lamport(1994)]{lamport94}
%   Leslie Lamport,
%   \textit{\LaTeX: a document preparation system},
%   Addison Wesley, Massachusetts,
%   2nd edition,
%   1994.

% \end{thebibliography}


\newpage

\section{Appendix A}



\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{l|c|c|c}
\toprule
\textbf{Parameters} & \textbf{Carbide Insert dataset} & \textbf{MvTec} & \textbf{KolektorSDD} \\
\midrule
Optimizer & AdamW & AdamW & SGD \\
\midrule
Learning Rate & 1e-3 & 1e-3 & 1e-3 \\
\midrule
Batch Size & 8 & 8 & 8 \\
\midrule
Iterations & 160000 & 15000 (CNN), 50000 (Transformer) & 15000 (CNN, Transformer) \\
\midrule
Loss & Focal Loss & Focal Loss & Focal Loss \\
\midrule
Seed & 42 & 42 & 42 \\
\bottomrule
\end{tabular}
}
\caption{Hyperparameters for Segmentation. The Focal Loss parameters are set to $\alpha=0.25$, $\gamma=2.0$, class\_weight=[0.1, 0.3, 0.6] for Carbide Insert Dataset and class\_weight=[0.1, 0.9] for Mvtec, KolektorSDD}
\label{tab:Hyperparameters_segmentation}
\end{table*}

\begin{table*}[h!]
\centering
\resizebox{1.0\textwidth}{!}{%
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{l|c}
\toprule
\textbf{Parameters} & \textbf{\hspace{3cm}Carbide Insert Dataset\hspace{3cm}} \\
\midrule
Optimizer & AdamW \\
\midrule
Learning Rate & 5e-3 \\
\midrule
Batch Size & 16 \\
\midrule
Epochs & 50 \\
\midrule
Loss & CrossEntropyLoss \\
\midrule
Seed & 42 \\
\bottomrule
\end{tabular}
}
\caption{Hyperparameters for Classification}
\label{tab:Hyperparameters_classification}
\end{table*}
\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\renewcommand{\arraystretch}{1.0}
\begin{tabular}{c|c|c|c|c|c}
\toprule
\multicolumn{2}{c|}{\textbf{Null Embedding Optimization}} & \multicolumn{2}{c|}{\textbf{Prompt-to-Prompt}} & \multicolumn{2}{c}{\textbf{RLI}} \\
\midrule
\textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} \\
\midrule
Inference Step & 50 & Cross Value & 0.8 & Alpha & 0.1 \\
Optimization Step & 10 & Self Value & 0.6 & & \\
&&Eq&2&&\\
\bottomrule
\end{tabular}
}
\caption{Parameters for Synthetic Data}
\label{tab:Hyperparameters_synthetic}
\end{table*}
\subsection{Experiments Setup}
We conducted experiments on various datasets for proving our methodology. To ensure reproducibility, the random seed was fixed to 42 for all experiments. Our experiments were conducted on three main tasks: defect segmentation, classification and creating synthetic data.
\subsubsection{Defect Segmentation}
For the segmentation task, we utilized three datasets: the Carbide Insert dataset, MvTec AD, and KolektorSDD. We employed the AdamW optimizer for the Carbide Insert and MvTec AD datasets, while Stochastic Gradient Descent (SGD) was used for KolektorSDD. The learning rate was set to $1 \times 10^{-3}$ across all datasets with a batch size of 8.The training iterations were adjusted based on the dataset complexity and model architecture. Specifically, the Carbide Insert dataset was trained for 160,000 iterations. For MvTec AD, CNN-based models were trained for 15,000 iterations, whereas Transformer-based models required 50,000 iterations for convergence. KolektorSDD models were trained for 15,000 iterations regardless of the architecture. To address the class imbalance problem inherent in defect detection, we adopted the Focal Loss with $\alpha=0.25$ and $\gamma=2.0$. The class weights were assigned as follows: $[0.1, 0.3, 0.6]$ for the Carbide Insert dataset, and $[0.1, 0.9]$ for both MvTec AD and KolektorSDD. Detailed hyperparameters are listed in Table \ref{tab:Hyperparameters_segmentation}.
\subsubsection{Defect Classification}
For the classification task, experiments were conducted on the Carbide Insert dataset. We used the AdamW optimizer with a learning rate of $5 \times 10^{-3}$. The model was trained for 50 epochs with a batch size of 16. We employed the standard Cross-Entropy Loss for optimization. The hyperparameters for the classification task are summarized in Table \ref{tab:Hyperparameters_classification}.
\subsubsection{Creating Synthetic Data}

To address the scarcity of defect data, we generated synthetic defect images using a diffusion-based editing approach. Our pipeline incorporates Null-text Inversion and Prompt-to-Prompt techniques, enhanced by our proposed RLI method.

\textbf{Inversion and Editing.}
We utilized Null-text Inversion to accurately reconstruct the input images, setting the inference steps to 50 and optimization steps to 10. For the editing process via Prompt-to-Prompt, we applied cross-attention injection during the first 80\% of the inference steps and self-attention injection during the first 60\%.

\textbf{RLI}
A key component of our method is RLI, which ensures the preservation of the original structural characteristics while selectively editing the target defect regions. By applying an interpolation factor $\alpha=0.1$ to the attention maps, RLI seamlessly blends the generated defect features into the background, maintaining the integrity of non-defect areas. The detailed hyperparameters for the synthetic data generation process are provided in Table \ref{tab:Hyperparameters_synthetic}.






\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.



